{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2736c3a-a69f-4ce5-abd6-44cdc964c51d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda7ef5-7db4-40a7-8020-18c4de8f1fe0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## log.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b94a197-dd8a-4b03-be94-80184f106588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace these everywhere\n",
    "def msginfo(verbose, msg):\n",
    "    msg2(verbose,msg)\n",
    "    \n",
    "def msg2(verbose, msg):\n",
    "    if verbose >= 2:\n",
    "        print(msg)\n",
    "\n",
    "def msg1(verbose, msg):\n",
    "    if verbose >= 1:\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476370f1-0da5-464b-99ff-1dbc2124d246",
   "metadata": {
    "tags": []
   },
   "source": [
    "## reference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bc2f3b4-b195-45df-b1df-5ec1b20da9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_gene_symbols(output_filepath = \"data/raw/gene_info.gz\", url = \"https://ftp.ncbi.nlm.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz\", verbose=0):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(output_filepath, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    msginfo(verbose, \"Download completed.\")\n",
    "\n",
    "def extract_gene_data(filepath = \"data/raw/gene_info.gz\"):\n",
    "    \"\"\" returns 3 sets \"\"\"\n",
    "    import gzip\n",
    "\n",
    "    gene_symbols = set()\n",
    "    dbxrefs = set()\n",
    "    gene_synonyms = set()\n",
    "\n",
    "    with gzip.open(filepath, \"rt\") as file:\n",
    "        for line in file:\n",
    "            if not line.startswith(\"#\"):\n",
    "                fields = line.strip().split(\"\\t\")\n",
    "                gene_symbols.add(fields[2])\n",
    "\n",
    "                # Extract dbXrefs\n",
    "                dbxref_field = fields[5]\n",
    "                if dbxref_field != \"-\":\n",
    "                    identifiers = dbxref_field.split(\"|\")\n",
    "                    dbxrefs.update(identifiers)\n",
    "\n",
    "                # Extract gene synonyms\n",
    "                synonym_field = fields[4]\n",
    "                if synonym_field != \"-\":\n",
    "                    synonyms = synonym_field.split(\"|\")\n",
    "                    gene_synonyms.update(synonyms)\n",
    "\n",
    "    return gene_symbols, dbxrefs, gene_synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a05d0c-3c05-4cb0-843b-684d5785e0b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## search_terms.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94eec2e-f95c-4f95-9ad6-a0c05868c0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_search_stop(search_file, stop_words):\n",
    "    # Load search terms from the file into a set\n",
    "    search_terms=[]\n",
    "    with open(search_file, 'r') as f:\n",
    "        search_terms = [line.strip() for line in f]\n",
    "    return search_terms\n",
    "\n",
    "def filter_search_terms(search_terms, stop_words):\n",
    "    filtered_terms = []\n",
    "    matched_stop_words = []\n",
    "    \n",
    "    # get the matched stop words and keep in original case\n",
    "    for term in search_terms:\n",
    "        if term.lower() in stop_words:\n",
    "            matched_stop_words.append(term)\n",
    "\n",
    "    # get the filtered terms, lower case them\n",
    "    filtered_terms = [term.lower() for term in search_terms if term.lower() not in stop_words]\n",
    "\n",
    "    # add the two lists together so the matched terms retain original case\n",
    "    # this will allow for finding genes that are also common english words\n",
    "    final_terms = filtered_terms + matched_stop_words\n",
    "    return (final_terms, filtered_terms, matched_stop_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630334e5-c301-4e23-91e5-763a17b3b2fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## stop_words.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ace36021-359a-4494-84ac-bb48109a68fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import string\n",
    "\n",
    "def fetch_brown_corpus():\n",
    "    from nltk.corpus import brown\n",
    "    corpus = 'brown'\n",
    "    # Select a specific category from the Brown Corpus for analysis\n",
    "    category = 'learned'\n",
    "    # Load the Brown Corpus\n",
    "    nltk.download('brown')\n",
    "    # Get the words from the selected category\n",
    "    words = brown.words(categories=category)\n",
    "    return words\n",
    "\n",
    "def read_search_stop(search_file, stop_words):\n",
    "    # Load search terms from the file into a set\n",
    "    search_terms=[]\n",
    "    with open(search_file, 'r') as f:\n",
    "        search_terms = [line.strip() for line in f]\n",
    "    return search_terms\n",
    "\n",
    "def filter_search_terms(search_terms, stop_words):\n",
    "    filtered_terms = []\n",
    "    matched_stop_words = []\n",
    "    \n",
    "    # get the matched stop words and keep in original case\n",
    "    for term in search_terms:\n",
    "        if term.lower() in stop_words:\n",
    "            matched_stop_words.append(term)\n",
    "\n",
    "    # get the filtered terms, lower case them\n",
    "    filtered_terms = [term.lower() for term in search_terms if term.lower() not in stop_words]\n",
    "\n",
    "    # add the two lists together so the matched terms retain original case\n",
    "    # this will allow for finding genes that are also common english words\n",
    "    final_terms = filtered_terms + matched_stop_words\n",
    "    return (final_terms, filtered_terms, matched_stop_words)\n",
    "\n",
    "def create_stop_words(frequency_list_outpath, custom_words) -> Set:\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    # Download the stopwords corpus\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    # Load the existing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Read the words from the frequency_list.txt file\n",
    "    with open(frequency_list_outpath, 'r') as file:\n",
    "        frequency_words = file.read().splitlines()\n",
    "\n",
    "    # Add the frequency words to the stop words set\n",
    "    stop_words.update(frequency_words)\n",
    "\n",
    "    # Add custom words\n",
    "    stop_words.update(custom_words)\n",
    "    return stop_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c5081c-b32e-4924-9097-b3452162f9cc",
   "metadata": {},
   "source": [
    "## fetch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1eb2b544-a7de-4452-bb23-8047f67fbe6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import urllib.request\n",
    "from urllib.error import URLError\n",
    "from shutil import rmtree\n",
    "\n",
    "def check_disk_space(predicted_size, download_dir, verbose):\n",
    "    required_space = predicted_size\n",
    "    available_space = os.statvfs(download_dir).f_frsize * os.statvfs(download_dir).f_bavail\n",
    "    required_space_human = subprocess.check_output(['numfmt', '--to=iec-i', '--suffix=B', str(required_space)]).decode().strip()\n",
    "    available_space_human = subprocess.check_output(['numfmt', '--to=iec-i', '--suffix=B', str(available_space)]).decode().strip()\n",
    "\n",
    "    msg1(verbose, f\"Predicted download size = {required_space_human}, Available space = {available_space_human}\")\n",
    "\n",
    "    if required_space > available_space:\n",
    "        print(f\"Insufficient disk space! Required: {required_space_human}, Available: {available_space_human}\")\n",
    "        exit(1)\n",
    "\n",
    "def download_file(url, file_path, verbose):\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "    except URLError as e:\n",
    "        msg1(verbose, f\"Error downloading file: {url}\")\n",
    "        msg1(verbose, f\"Reason: {str(e.reason)}\")\n",
    "        #exit(1)\n",
    "\n",
    "def verify_md5(file_path, md5_file_path, verbose):\n",
    "    try:\n",
    "        output = subprocess.check_output(['md5sum', '-c', os.path.basename(md5_file_path)], cwd=os.path.dirname(md5_file_path), stderr=subprocess.DEVNULL).decode()\n",
    "        #output = subprocess.check_output(['md5sum', '-c', md5_file_path], stderr=subprocess.DEVNULL).decode()\n",
    "        if \"OK\" in output:\n",
    "            msg2(verbose, f\"{md5_file_path}: OK - MD5 checksum verification succeeded.\")\n",
    "        else:\n",
    "            msg1(verbose, f\"ERROR: {md5_file_path}: FAILED - MD5 checksum verification failed.\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        msg1(verbose, f\"ERROR: {md5_file_path}: FAILED - MD5 checksum verification failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c1dee-2971-4d32-85fc-801ee3aafd85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## parse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b8b228e-ae5c-467a-b2e5-e1c1af94c708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def extract_data(element):\n",
    "    data = {}\n",
    "    data['PMID'] = element.findtext('MedlineCitation/PMID')\n",
    "    data['Title'] = element.findtext('MedlineCitation/Article/ArticleTitle')\n",
    "    data['Abstract'] = element.findtext('MedlineCitation/Article/Abstract/AbstractText')\n",
    "    data['Journal'] = element.findtext('MedlineCitation/Article/Journal/Title')\n",
    "    data['PublicationDate'] = element.findtext('MedlineCitation/Article/Journal/JournalIssue/PubDate/Year')\n",
    "    data['JournalTitle'] = element.findtext('MedlineCitation/Article/Journal/Title')\n",
    "    data['ArticleType'] = element.findtext('MedlineCitation/Article/PublicationTypeList/PublicationType')\n",
    "    \n",
    "    # Extract the descriptor names and qualifier names from the XML\n",
    "    mesh_headings = element.findall('.//MeshHeading')\n",
    "    mesh_heading_list = []\n",
    "    for heading in mesh_headings:\n",
    "        descriptor_name = heading.findtext('DescriptorName')\n",
    "        qualifier_names = [qualifier.text for qualifier in heading.findall('QualifierName')]\n",
    "        mesh_heading_list.append(descriptor_name)\n",
    "        mesh_heading_list.extend(qualifier_names)\n",
    "    data['MeshHeadingList'] = ','.join(mesh_heading_list)\n",
    "                                        \n",
    "    publication_types = element.findall('MedlineCitation/Article/PublicationTypeList/PublicationType')\n",
    "    data['PublicationTypeList'] = \",\".join([ptype.text for ptype in publication_types])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def prune_df(df, length_threshold = 405, verbose=2):\n",
    "    # exclude articles with no abstract, no date, or abstracts that are too short (less than length_threshold letters)\n",
    "    pruned_df = df[df['Abstract'].notna() & df['PublicationDate'].notna()]\n",
    "\n",
    "    # cut out any short articles\n",
    "    all_pruned = len(pruned_df)\n",
    "    msg2(verbose, f\"Number of all abstracts before pruning short articles = {all_pruned}\")\n",
    "    pruned_df = pruned_df[pruned_df['Abstract'].str.len() >= length_threshold]\n",
    "    long_pruned = len(pruned_df)\n",
    "    msg2(verbose, f\"Number after pruning short articles = {long_pruned}\")\n",
    "    msg2(verbose, f\"Number discarded for being too short: {all_pruned - long_pruned}\")\n",
    "\n",
    "    return pruned_df\n",
    "\n",
    "def get_pub_df(filename, inpath, outpath, length_threshold, prune=True,verbose=0):\n",
    "    import gzip\n",
    "    import xml.etree.ElementTree as ET\n",
    "    import pandas as pd\n",
    "\n",
    "    pubmed_filepath = os.path.join(inpath, filename)\n",
    "    # Open the gzip'd XML file\n",
    "    with gzip.open(pubmed_filepath, 'rb') as f:\n",
    "        # Read the contents of the gzip'd file\n",
    "        gzip_content = f.read()\n",
    "\n",
    "    # Parse the XML content using ElementTree\n",
    "    root = ET.fromstring(gzip_content)\n",
    "\n",
    "    # Extract data from each article and store in a list\n",
    "    articles = []\n",
    "    for article in root.findall('.//PubmedArticle'):\n",
    "        articles.append(extract_data(article))\n",
    "\n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(articles)\n",
    "    df = df.drop_duplicates()\n",
    "    if prune:\n",
    "        msg2(verbose, f\"Number of all articles:{len(df)}\")\n",
    "        df = prune_df(df, length_threshold = length_threshold, verbose=verbose)\n",
    "        msg2(verbose, f\"Number of pruned articles:{len(df)}\")\n",
    "    \n",
    "    # convert objects to simple types\n",
    "    df['PublicationDate'] = df['PublicationDate'].astype(int)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed812676-3e93-44da-b133-27b33abe86bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1debca71-afe6-4035-afc2-da56ffc8e61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Set\n",
    "from pydantic import BaseModel, root_validator\n",
    "\n",
    "class ReferenceData(BaseModel):\n",
    "    \"\"\"\n",
    "    Files for retrieving and transforming reference gene information\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ReferenceData, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.version_root = os.path.join(os.getcwd(), str(self.version), self.data_root)\n",
    "        msg2(self.verbose, f\"version_root={self.version_root}\")\n",
    "        \n",
    "        # make directory structure\n",
    "        os.makedirs(self.version_root, exist_ok=True)\n",
    "        os.makedirs(self.raw_path(), exist_ok=True)\n",
    "        os.makedirs(self.reference_path(), exist_ok=True)\n",
    "        os.makedirs(self.dbxref_path(), exist_ok=True)\n",
    "        os.makedirs(self.search_path(), exist_ok=True)\n",
    "        os.makedirs(self.pub_inpath(), exist_ok=True)\n",
    "        os.makedirs(self.pub_outpath(), exist_ok=True)\n",
    "        os.makedirs(self.genes_outpath(), exist_ok=True)\n",
    "        \n",
    "        msg2(self.verbose, \"Created directory structure.\")\n",
    "\n",
    "    ncbi_gene_info_url: str = \"https://ftp.ncbi.nlm.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz\"\n",
    "    \n",
    "    data_root: str  = \"data/\"\n",
    "    \"\"\" full path to where all the raw, reference, and generated data are stored \"\"\"\n",
    "    \n",
    "    raw_data_path: str = \"raw/\"\n",
    "    \n",
    "    reference_data_path: str = \"reference/\"\n",
    "    \n",
    "    dbxref_reference_data_path: str = \"dbxrefs/\"\n",
    "    \n",
    "    dbxrefs: List = [] \n",
    "    \"\"\" if empty, it will just get filled with a list of all the files created in the m.dbxref_path() \"\"\"\n",
    "        \n",
    "    gene_info_filename: str = \"gene_info.gz\"\n",
    "    \n",
    "    gene_symbols_filename: str = \"gene_symbols.txt\"\n",
    "    \n",
    "    gene_synonyms_filename: str = \"gene_synonyms.txt\"\n",
    "    \n",
    "    search_terms_path: str = \"search_terms/\"\n",
    "    \n",
    "    frequency_list_filename: str = \"frequency_list.txt\"\n",
    "    \n",
    "    corpus_stop_word_list_length:int = 4000\n",
    "    \n",
    "    # old lsit\n",
    "    #custom_stop_words:List = ['ago', 'aim', 'amid', 'april', 'arch', 'bed', 'bite', 'bug', 'co', 'crop', 'damage', 'et', \n",
    "    #                'fast', 'fat', 'fate', 'gap', 'genesis', 'ii', 'iv', 'lamp', 'laser', 'mater', 'melt', 'mice', 'minor', 'mv', 'net', \n",
    "    #                'not', 'race', 'rank', 'se', 'sink', 'soft', 'spatial', 'steel', 'stop', 'tau', 'traits', 'via']\n",
    "    \n",
    "    # may overlap somewhat with stop words\n",
    "    custom_stop_words:List = ['ago', 'aim', 'amid', 'april', 'arch', 'bed', 'bite', 'bug', 'cage', 'co', 'crop',\n",
    "                    'damage', 'danger', 'digit', 'et', 'fast', 'fat', 'fate', 'fire', 'flower', 'gap', 'genesis', 'gov', 'gpa', 'grasp',\n",
    "                    'ii', 'inos', 'iv', 'killer', 'lab', 'lamp', 'laser', 'map', 'mask', 'mater', 'melt', 'mice', 'minor', 'miss', 'mv',\n",
    "                    'nail', 'net', 'not', 'osf', 'pan', 'par', 'pha', 'rab', 'race', 'rain', 'rank',\n",
    "                    'san', 'sand', 'se', 'sink', 'soft', 'spatial', 'spin', 'spp', 'steel', 'stop',\n",
    "                    'storm', 'tactile', 'tau', 'theta', 'tip', 'traits', 'via']\n",
    "\n",
    "    search_terms_filename: str = \"search_terms.txt\"\n",
    "    \n",
    "    filtered_terms_filename: str = \"filtered_terms.txt\"\n",
    "    \n",
    "    abstract_inpath: str = \"pubs/\"\n",
    "    \n",
    "    refresh_abstract_xml_files: bool = False\n",
    "    \"\"\" Set to True to overwrite downloaded NCBI XML abstract files and checksum files \"\"\"\n",
    "    \n",
    "    num_abstract_xml_files: int\n",
    "    \"\"\" Number of NCBI XML files to download; Set this to -1 to get all abstracts \"\"\"\n",
    "    \n",
    "    abstract_outpath: str = \"csvpubs/\"\n",
    "\n",
    "    abstract_length_threshold: int = 405\n",
    "\n",
    "    abstract_genes_outpath: str = \"genes/\"\n",
    "\n",
    "    \n",
    "    #path functions\n",
    "    def raw_path(self):\n",
    "        return os.path.join(os.getcwd(), self.version_root, self.raw_data_path)\n",
    "    def reference_path(self):\n",
    "        return os.path.join(os.getcwd(), self.version_root, self.reference_data_path)\n",
    "    def dbxref_path(self):\n",
    "        return os.path.join(os.getcwd(), self.version_root, self.reference_data_path, self.dbxref_reference_data_path)\n",
    "    def search_path(self):\n",
    "        return os.path.join(os.getcwd(), self.version_root, self.search_terms_path)\n",
    "    def pub_inpath(self):\n",
    "        return os.path.join(os.getcwd(), self.version_root, self.raw_data_path, self.abstract_inpath)\n",
    "    def pub_outpath(self):\n",
    "        return os.path.join(os.getcwd(), self.version_root, self.abstract_outpath)\n",
    "    def genes_outpath(self):\n",
    "        return os.path.join(self.pub_outpath(), self.abstract_genes_outpath)\n",
    "\n",
    "    verbose: int = 0\n",
    "    \"\"\" 0 prints nothing, 1 prints errors and warnings, 2 prints info \"\"\"\n",
    "    \n",
    "    debug: bool = False\n",
    "    \"\"\" Prints very detailed debuggin messages \"\"\"\n",
    "    \n",
    "    version: str = None\n",
    "    \"\"\" If None, version will e set to timestamp \"\"\"\n",
    "    \n",
    "    rand_seed: int = 42\n",
    "    \"\"\" if None, reproducible within the same release\"\"\"\n",
    "    \n",
    "    version_root: str = None\n",
    "    \"\"\" leave this alone it will be computed as data_root/version \"\"\"\n",
    "    \n",
    "    \n",
    "    @root_validator(pre=False, skip_on_failure=True)\n",
    "    def valid_paths_for_data_tree(cls, v: Dict) -> Dict:\n",
    "        data_root = v.get(\"data_root\")\n",
    "        version = v.get(\"version\")\n",
    "        \n",
    "        import os\n",
    "        \n",
    "        if version is None:\n",
    "            import subprocess\n",
    "            \n",
    "            # use miliseconds and create new directory structure\n",
    "            version = str(subprocess.check_output([\"date\", \"+%1N.%S_%M_%H_%Y_%m_%d\"]).strip().decode())\n",
    "            v[\"version\"] = version\n",
    "        \n",
    "        if data_root is None:\n",
    "            raise Exception(\"Error: data_root cannot be None.\")\n",
    "            \n",
    "        version_root = os.path.join(os.getcwd(), data_root, str(version))\n",
    "        v[\"version_root\"] = version_root\n",
    "        \n",
    "        if os.path.exists(version_root):\n",
    "            msg2(f\"Using existing data director {version_root}\")\n",
    "            \n",
    "        return v\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd46a5c-ef8c-4b16-91b1-d14186ebaf97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## api.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1959fdd2-3123-4f8f-9da2-2a572217342c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_gene_reference_data(m: ReferenceData):\n",
    "    \n",
    "    raw_gene_info_filepath = os.path.join(m.raw_path(), m.gene_info_filename)\n",
    "    reference_gene_symbols_filepath = os.path.join(m.reference_path(), m.gene_symbols_filename)\n",
    "    reference_gene_synonyms_filepath = os.path.join(m.reference_path(), m.gene_synonyms_filename)\n",
    "    dbxref_path = m.dbxref_path()\n",
    "    verbose = m.verbose\n",
    "    url = m.ncbi_gene_info_url\n",
    "    \n",
    "    # Download the gene symbols file\n",
    "    download_gene_symbols(output_filepath = raw_gene_info_filepath, url = url, verbose = verbose)\n",
    "\n",
    "    # Extract gene data\n",
    "    gene_symbols, dbxrefs, gene_synonyms = extract_gene_data(filepath = raw_gene_info_filepath)\n",
    "\n",
    "    # Save gene symbols to a file\n",
    "    with open(reference_gene_symbols_filepath, \"w\") as file:\n",
    "        for symbol in gene_symbols:\n",
    "            file.write(symbol + \"\\n\")\n",
    "\n",
    "    msg2(verbose, f\"Gene symbols saved to {reference_gene_symbols_filepath}\")\n",
    "\n",
    "    # Save dbXrefs to separate files\n",
    "    for identifier in dbxrefs:\n",
    "            identifier_parts = identifier.split(\":\")\n",
    "            identifier_type = identifier_parts[0].replace('/','_')\n",
    "            identifier_value = \":\".join(identifier_parts[1:])\n",
    "            filename = f\"{dbxref_path}/{identifier_type}.txt\"\n",
    "            with open(filename, \"a\") as file:\n",
    "                file.write(identifier_value + \"\\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    for identifiers in dbxrefs:\n",
    "        for identifier in identifiers:\n",
    "            identifier_parts = identifier.split(\":\")\n",
    "            identifier_type = identifier_parts[0].replace('/','_')\n",
    "            identifier_value = \":\".join(identifier_parts[1:])\n",
    "            filename = f\"{dbxref_path}/{identifier_type}.txt\"\n",
    "            with open(filename, \"a\") as file:\n",
    "                file.write(identifier_value + \"\\n\")\n",
    "    \"\"\"\n",
    "    msg2(verbose, \"dbXrefs saved to individual files.\")\n",
    "\n",
    "    # Save gene synonyms to a file\n",
    "    with open(reference_gene_synonyms_filepath, \"w\") as file:\n",
    "        for synonym in gene_synonyms:\n",
    "            file.write(synonym + \"\\n\")\n",
    "\n",
    "    msg2(verbose, f\"Gene synonyms saved to {reference_gene_synonyms_filepath}\")\n",
    "\n",
    "def create_frequency_list(m: ReferenceData) -> List:\n",
    "    \n",
    "    frequency_list_outpath = os.path.join(m.search_path(), m.frequency_list_filename)\n",
    "    stop_word_list_length = m.corpus_stop_word_list_length\n",
    "    verbose = m.verbose\n",
    "    \n",
    "    import nltk\n",
    "    from nltk import FreqDist\n",
    "    import string\n",
    "\n",
    "    words = fetch_brown_corpus()\n",
    "\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = [word.lower() for word in words if word not in string.punctuation and word.isalnum()]\n",
    "\n",
    "    # Compute the frequency distribution of words\n",
    "    freq_dist = FreqDist(words)\n",
    "\n",
    "    # Get the most frequent words\n",
    "    most_common_words = freq_dist.most_common(stop_word_list_length)\n",
    "\n",
    "    # Write the frequency list to a file\n",
    "    with open(frequency_list_outpath, 'w') as file:\n",
    "        for word, frequency in most_common_words:\n",
    "            file.write(word + '\\n')\n",
    "    msg2(verbose, f\"Wrote {frequency_list_outpath}\")\n",
    "    return most_common_words\n",
    "\n",
    "def create_search_terms_file(m: ReferenceData):\n",
    "    import os\n",
    "\n",
    "    dbxrefs = m.dbxrefs\n",
    "    dbxrefs_path = m.dbxref_path()  \n",
    "    gene_symbols_filepath = os.path.join(m.reference_path(), m.gene_symbols_filename)\n",
    "    gene_synonyms_filepath = os.path.join(m.reference_path(), m.gene_synonyms_filename)\n",
    "    search_terms_filepath = os.path.join(m.search_path(), m.search_terms_filename)\n",
    "    verbose = m.verbose\n",
    "\n",
    "    if dbxrefs == []:\n",
    "        # Get a list of all files in the directory\n",
    "        dbxrefs = os.listdir(dbxrefs_path)\n",
    "        # Filter out directories from the list\n",
    "        dbxrefs = [f for f in dbxrefs if os.path.isfile(os.path.join(dbxrefs_path, f))]\n",
    "  \n",
    "    \n",
    "    with open(f\"{search_terms_filepath}.unsorted\", \"w\") as outfile:\n",
    "        for ref in dbxrefs:\n",
    "            with open(os.path.join(dbxrefs_path, ref)) as infile:\n",
    "                sorted_lines = sorted(set(infile.readlines()))\n",
    "                outfile.writelines(sorted_lines)\n",
    "                #outfile.write(infile.read())\n",
    "\n",
    "        with open(gene_symbols_filepath) as infile:\n",
    "            sorted_lines = sorted(set(infile.readlines()))\n",
    "            outfile.writelines(sorted_lines)\n",
    "            #outfile.write(infile.read())\n",
    "\n",
    "        with open(gene_synonyms_filepath) as infile:\n",
    "            sorted_lines = sorted(set(infile.readlines()))\n",
    "            outfile.writelines(sorted_lines)\n",
    "            #outfile.write(infile.read())\n",
    "\n",
    "    # Sort and remove duplicates from the search terms file\n",
    "    search_terms_unsorted_filepath = f\"{search_terms_filepath}.unsorted\"\n",
    "    os.system(f\"sort -u {search_terms_unsorted_filepath} | grep -v not > {search_terms_filepath}\")\n",
    "\n",
    "    msg2(verbose, f\"Created {search_terms_filepath}.\")\n",
    "    msg2(verbose, f\"Created {search_terms_unsorted_filepath} - can be removed.\")\n",
    "    line_count = sum(1 for line in open(search_terms_filepath))\n",
    "    msg2(verbose, f\"Number of lines in {search_terms_filepath}: {line_count}\")\n",
    "\n",
    "def create_filtered_search_terms(m: ReferenceData) -> List:\n",
    "    \n",
    "    search_file = os.path.join(m.search_path(), m.search_terms_filename)\n",
    "    frequency_list_outpath = os.path.join(m.search_path(), m.frequency_list_filename)\n",
    "    custom_words = m.custom_stop_words\n",
    "    final_file = os.path.join(m.search_path(), m.filtered_terms_filename)\n",
    "    verbose = m.verbose\n",
    "    \n",
    "    stop_words = create_stop_words(frequency_list_outpath, custom_words)\n",
    "    \n",
    "    search_terms = read_search_stop(search_file = search_file, stop_words = stop_words)\n",
    "    msg2(verbose, f\"Number of original search_terms:{len(search_terms)}\")\n",
    "    final_terms, filtered_terms, matched_stop_words = filter_search_terms(search_terms, stop_words)\n",
    "    msg2(verbose, f\"number of filtered_terms:{len(filtered_terms)}\\nfinal number of final_terms:{len(final_terms)}\\n number of matched_stop_words:{len(matched_stop_words)}\\nmatched_stop_words={matched_stop_words}\")\n",
    "    if final_file is not None:\n",
    "        with open(final_file, \"w\") as f:\n",
    "            f.writelines('\\n'.join(final_terms))\n",
    "    msg2(verbose, f\"Created {final_file}\")\n",
    "    return final_terms\n",
    "\n",
    "\n",
    "def fetch_abstracts(m: ReferenceData):\n",
    "    \n",
    "    num_files = m.num_abstract_xml_files\n",
    "    refresh = m.refresh_abstract_xml_files\n",
    "    download_dir = m.pub_inpath()\n",
    "    verbose = m.verbose\n",
    "    \n",
    "    \"\"\" This can probably be done faster with download_files.sh \"\"\" \n",
    "    msg2(verbose, f\"Download Directory: {download_dir}\")\n",
    "    msg2(verbose, f\"Number of abstracts to ensure have been downloaded: {num_files}\")\n",
    "    msg2(verbose, f\"Refresh: {refresh}\")\n",
    "\n",
    "    # FTP settings\n",
    "    ftp_host = \"ftp.ncbi.nlm.nih.gov\"\n",
    "    ftp_path = \"/pubmed/baseline/\"\n",
    "\n",
    "    # Retrieve file names and find the largest number\n",
    "    #file_list = subprocess.check_output(['curl', '-s', f\"ftp://{ftp_host}{ftp_path}\"]).decode().splitlines()\n",
    "    \n",
    "    output = subprocess.check_output(['curl', '-s', f\"ftp://{ftp_host}{ftp_path}\"]).decode()\n",
    "    file_list = [line.split()[-1] for line in output.splitlines() if line.endswith(\".xml.gz\")]\n",
    "\n",
    "    msg2(verbose, f\"Total number of NCBI abstract XML files: {len(file_list)}\")\n",
    "    latest_files = [file_name for file_name in file_list if file_name.startswith(\"pubmed23n\") and file_name.endswith(\".xml.gz\")]\n",
    "    latest_files.sort(reverse=True)\n",
    "    latest_files = latest_files[:num_files]\n",
    "    msg2(verbose, f\"latest_files {num_files}: {latest_files}\")\n",
    "\n",
    "    # Check if enough files are available\n",
    "    if len(latest_files) == 0:\n",
    "        msg1(verbose, \"Error: Insufficient number of files available!\")\n",
    "        exit(1)\n",
    "\n",
    "    # Calculate total predicted size\n",
    "    total_size = 0\n",
    "    for file_name in latest_files:\n",
    "        response = subprocess.check_output(['curl', '-sI', f\"ftp://{ftp_host}{ftp_path}{file_name}\"]).decode()\n",
    "        file_size = int(response.split(\"Content-Length: \")[1].split(\"\\r\")[0])\n",
    "        total_size += file_size\n",
    "\n",
    "    # Check disk space before downloading\n",
    "    check_disk_space(total_size, download_dir, verbose=verbose)\n",
    "\n",
    "    # Download and check files\n",
    "    for file_name in latest_files:\n",
    "        md5_file_name = f\"{file_name}.md5\"\n",
    "        file_path = os.path.join(download_dir, file_name)\n",
    "        md5_file_path = os.path.join(download_dir, md5_file_name)\n",
    "\n",
    "        # Refresh files that were previously downloaded?\n",
    "        if not refresh:\n",
    "            # No, so skip downloading those again\n",
    "\n",
    "            # If one file or the other is missing, you still have to do a download\n",
    "            # Here, just provide information as to which files are present.\n",
    "            if os.path.isfile(file_path) and not os.path.isfile(md5_file_path):\n",
    "                msg1(verbose, f\"ERROR: Missing - {md5_file_path}; re-downloading now\")\n",
    "            if not os.path.isfile(file_path) and os.path.isfile(md5_file_path):\n",
    "                msg1(verbose, f\"ERROR: Missing - {file_path}; re-downloading now\")\n",
    "\n",
    "            if os.path.isfile(file_path) and os.path.isfile(md5_file_path):\n",
    "                msg1(verbose, f\"SKIP: {file_path} exists.\")\n",
    "                continue\n",
    "\n",
    "        # Check file size\n",
    "        response = subprocess.check_output(['curl', '-sI', f\"ftp://{ftp_host}{ftp_path}{file_name}\"]).decode()\n",
    "        file_size = int(response.split(\"Content-Length: \")[1].split(\"\\r\")[0])\n",
    "\n",
    "        msg2(verbose, f\"File: {file_name}, Size: {file_size} bytes\")\n",
    "\n",
    "        # Download file\n",
    "        msg2(verbose, f\"WARNING: Downloading: {file_name} to {download_dir}\")\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        download_file(f\"ftp://{ftp_host}{ftp_path}{file_name}\", file_path, verbose)\n",
    "\n",
    "        # Download MD5 file\n",
    "        if os.path.isfile(md5_file_path):\n",
    "            os.remove(md5_file_path)\n",
    "        download_file(f\"ftp://{ftp_host}{ftp_path}{md5_file_name}\", md5_file_path, verbose)\n",
    "\n",
    "        # Check MD5\n",
    "        verify_md5(file_path, md5_file_path, verbose)\n",
    "\n",
    "    total_size_human = subprocess.check_output(['numfmt', '--to=iec-i', '--suffix=B', str(total_size)]).decode().strip()\n",
    "    msg2(verbose, f\"Total size of abstract files: {total_size_human}\")\n",
    "\n",
    "def create_pubcsv_dataset(m: ReferenceData) -> List:\n",
    "    \"\"\" Takes about 14min for 30 (2 per minute) \"\"\"\n",
    "    \n",
    "    abstract_length_threshold = m.abstract_length_threshold\n",
    "    pub_inpath = m.pub_inpath()\n",
    "    pub_outpath = m.pub_outpath()\n",
    "    verbose = m.verbose\n",
    "\n",
    "    import os\n",
    "    import glob\n",
    "    \n",
    "    csv_list = []\n",
    "    # Iterate through files in the directory\n",
    "    for filepath in glob.glob(os.path.join(pub_inpath, \"pubmed*.xml.gz\")):\n",
    "        msg2(verbose, f\"Converting file {filepath}\")\n",
    "        if os.path.isfile(filepath):\n",
    "            filename = os.path.basename(filepath)\n",
    "            df = get_pub_df(filename=filename, inpath=pub_inpath, outpath= pub_outpath, prune=True, length_threshold = abstract_length_threshold, verbose = verbose)\n",
    "            csv_filepath = os.path.join(pub_outpath, f\"{filename}.csv\")\n",
    "            df.to_csv(csv_filepath, header=False, index=False, sep=\"\\t\")\n",
    "            msg2(verbose, f\"Wrote file:{csv_filepath}\")\n",
    "            csv_list.append(csv_filepath)\n",
    "            \n",
    "    return(csv_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e9bb0-793d-4bc1-86cc-c4d32b2b21f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "095a3b77-efa5-412e-b346-94ec51164085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version_root=/home/krobasky/prompt/end2end/v1/data/\n",
      "Created directory structure.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ReferenceData(ncbi_gene_info_url='https://ftp.ncbi.nlm.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz', data_root='data/', raw_data_path='raw/', reference_data_path='reference/', dbxref_reference_data_path='dbxrefs/', dbxrefs=['AllianceGenome.txt', 'Ensembl.txt', 'HGNC.txt', 'IMGT_GENE-DB.txt'], gene_info_filename='gene_info.gz', gene_symbols_filename='gene_symbols.txt', gene_synonyms_filename='gene_synonyms.txt', search_terms_path='search_terms/', frequency_list_filename='frequency_list.txt', corpus_stop_word_list_length=4000, custom_stop_words=['ago', 'aim', 'amid', 'april', 'arch', 'bed', 'bite', 'bug', 'cage', 'co', 'crop', 'damage', 'danger', 'digit', 'et', 'fast', 'fat', 'fate', 'fire', 'flower', 'gap', 'genesis', 'gov', 'gpa', 'grasp', 'ii', 'inos', 'iv', 'killer', 'lab', 'lamp', 'laser', 'map', 'mask', 'mater', 'melt', 'mice', 'minor', 'miss', 'mv', 'nail', 'net', 'not', 'osf', 'pan', 'par', 'pha', 'rab', 'race', 'rain', 'rank', 'san', 'sand', 'se', 'sink', 'soft', 'spatial', 'spin', 'spp', 'steel', 'stop', 'storm', 'tactile', 'tau', 'theta', 'tip', 'traits', 'via'], search_terms_filename='search_terms.txt', filtered_terms_filename='filtered_terms.txt', abstract_inpath='pubs/', refresh_abstract_xml_files=False, num_abstract_xml_files=5, abstract_outpath='csvpubs/', abstract_length_threshold=405, abstract_genes_outpath='genes/', verbose=2, debug=False, version='v1', rand_seed=42, version_root='/home/krobasky/prompt/end2end/v1/data/')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exclue miRNA and MIM\n",
    "m = ReferenceData(version = \"v1\", verbose = 2, \n",
    "                  num_abstract_xml_files = 5,\n",
    "                  dbxrefs = [\"AllianceGenome.txt\", \"Ensembl.txt\", \"HGNC.txt\", \"IMGT_GENE-DB.txt\"]  )\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6bc688-8a40-4e13-961e-e92e61e4ab8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get human genes list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21a65e54-03ee-4e02-a4da-3dced06a6856",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed.\n",
      "Gene symbols saved to /home/krobasky/prompt/end2end/v1/data/reference/gene_symbols.txt\n",
      "dbXrefs saved to individual files.\n",
      "Gene synonyms saved to /home/krobasky/prompt/end2end/v1/data/reference/gene_synonyms.txt\n"
     ]
    }
   ],
   "source": [
    "create_gene_reference_data(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f9b4ec-6bb4-4686-b924-3d536a2c1f28",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create filtered_terms.txt - 3 API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea086ba4-4845-4e6c-8e2b-4fe6053517e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/krobasky/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /home/krobasky/prompt/end2end/v1/data/search_terms/frequency_list.txt\n"
     ]
    }
   ],
   "source": [
    "_ = create_frequency_list(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "834baef4-34aa-47cc-9fa6-531686db9f81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /home/krobasky/prompt/end2end/v1/data/search_terms/search_terms.txt.\n",
      "Created /home/krobasky/prompt/end2end/v1/data/search_terms/search_terms.txt.unsorted - can be removed.\n",
      "Number of lines in /home/krobasky/prompt/end2end/v1/data/search_terms/search_terms.txt: 338143\n"
     ]
    }
   ],
   "source": [
    "create_search_terms_file(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "338a065c-0fb9-4f40-8ea3-8d380f002aec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/krobasky/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original search_terms:338143\n",
      "number of filtered_terms:337951\n",
      "final number of final_terms:338143\n",
      " number of matched_stop_words:192\n",
      "matched_stop_words=['ABO', 'ACE', 'ACT', 'AF', 'AGO', 'AID', 'AIM', 'AIR', 'ALL', 'AM', 'AMID', 'AN', 'APRIL', 'APT', 'ARC', 'ARCH', 'ARM', 'ARMS', 'ART', 'AS', 'ASK', 'AT', 'BAD', 'BANK', 'BASE', 'BED', 'BEST', 'BITE', 'BOD', 'BORIS', 'BRIGHT', 'BUG', 'CAGE', 'CALL', 'CAN', 'CAR', 'CAT', 'CELL', 'CHIP', 'CO', 'CROP', 'DAMAGE', 'DANGER', 'DC', 'DIGIT', 'DO', 'END', 'ET', 'ETA', 'FACE', 'FACT', 'FAST', 'FAT', 'FATE', 'FIND', 'FIRE', 'FLOWER', 'FOR', 'GAP', 'GAS', 'Genesis', 'GET', 'GO', 'GOV', 'GPA', 'GRASP', 'GREAT', 'H', 'HAD', 'HAS', 'HE', 'hELD', 'HIS', 'hole', 'HOT', 'HR', 'iCE', 'ICE', 'IF', 'II', 'IMPACT', 'IN', 'INOS', 'IV', 'JET', 'KILLER', 'LAB', 'LAMP', 'LARGE', 'LASER', 'LED', 'LIGHT', 'LIME', 'LIMIT', 'MA', 'MAIL', 'MAP', 'MARCH', 'MARK', 'MARS', 'MASK', 'MASS', 'MATER', 'ME', 'MELT', 'MEN', 'Met', 'MET', 'MG', 'MICE', 'MINOR', 'MISS', 'ML', 'MV', 'NAIL', 'NEST', 'NET', 'NOT', 'NS', 'ODD', 'OF', 'ON', 'OSF', 'OUT', 'P', 'PACE', 'PAINT', 'PAN', 'PAR', 'PARTICLE', 'PAST', 'PBS', 'PER', 'PERMIT', 'PH', 'PHA', 'PILOT', 'PIP', 'PLANE', 'POEM', 'POST', 'RAB', 'RACE', 'RAIN', 'RANK', 'RED', 'REST', 'RH', 'SAN', 'SAND', 'SE', 'SECRET', 'SERA', 'SET', 'SHARP', 'SHE', 'SHOT', 'SIMPLE', 'SINK', 'SOFT', 'SPATIAL', 'SPIN', 'SPOT', 'SPP', 'SPRING', 'STEEL', 'STEP', 'STOP', 'STORM', 'T', 'TACTILE', 'TAPE', 'TASK', 'TAU', 'TAX', 'THETA', 'TIP', 'TO', 'TOP', 'TRADE', 'TRAIL', 'TRAITS', 'TrIP', 'TRIP', 'TUBE', 'UP', 'VAN', 'VIA', 'WAS', 'WASH', 'WAVE', 'WISH']\n",
      "Created /home/krobasky/prompt/end2end/v1/data/search_terms/filtered_terms.txt\n"
     ]
    }
   ],
   "source": [
    "final_terms = create_filtered_search_terms(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34ebce6d-5312-4d69-82ca-280f9e756963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338143"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a1fa2-7e36-4ddd-a239-0a373d79be1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fetch ncbi article zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "778d3c6e-2de6-402e-9c1f-64ecc2f93926",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Directory: /home/krobasky/prompt/end2end/v1/data/raw/pubs/\n",
      "Number of abstracts to ensure have been downloaded: 10\n",
      "Refresh: False\n",
      "Total number of NCBI abstract XML files: 1166\n",
      "latest_files 10: ['pubmed23n1166.xml.gz', 'pubmed23n1165.xml.gz', 'pubmed23n1164.xml.gz', 'pubmed23n1163.xml.gz', 'pubmed23n1162.xml.gz', 'pubmed23n1161.xml.gz', 'pubmed23n1160.xml.gz', 'pubmed23n1159.xml.gz', 'pubmed23n1158.xml.gz', 'pubmed23n1157.xml.gz']\n",
      "Predicted download size = 610MiB, Available space = 111GiB\n",
      "SKIP: /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1166.xml.gz exists.\n",
      "SKIP: /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1165.xml.gz exists.\n",
      "SKIP: /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1164.xml.gz exists.\n",
      "SKIP: /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1163.xml.gz exists.\n",
      "SKIP: /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1162.xml.gz exists.\n",
      "SKIP: /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1161.xml.gz exists.\n",
      "SKIP: /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1160.xml.gz exists.\n",
      "SKIP: /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1159.xml.gz exists.\n",
      "SKIP: /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1158.xml.gz exists.\n",
      "SKIP: /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1157.xml.gz exists.\n",
      "Total size of abstract files: 610MiB\n"
     ]
    }
   ],
   "source": [
    "# 60GB is needed to get all files\n",
    "# 2 min/file ... ~ 2 days to get 'em all\n",
    "m.num_abstract_xml_files=10\n",
    "fetch_abstracts(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c808fdc6-67f2-4b35-957b-83b30eba667a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%bash\n",
    "# this would probably be faster, but harder to maintain\n",
    "#VERSION_ROOT=v1/data\n",
    "#VERBOSE=1\n",
    "#./gpubs/scripts/download_pubs.sh -n 5 -d ${VERSION_ROOT}/raw/pubs -v ${VERBOSE} 2> download.err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07692a6d-69cd-4806-8f7a-82719db2cb02",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create CSVs from XMLs\n",
    "functions in pubmed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "96fc8b0b-9658-41f6-8a31-2b85ea8c5e85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1160.xml.gz\n",
      "Number of all articles:29995\n",
      "Number of all abstracts before pruning short articles = 27392\n",
      "Number after pruning short articles = 18053\n",
      "Number discarded for being too short: 9339\n",
      "Number of pruned articles:18053\n",
      "Wrote file:/home/krobasky/prompt/end2end/v1/data/csvpubs/pubmed23n1160.xml.gz.csv\n",
      "Converting file /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1165.xml.gz\n",
      "Number of all articles:29996\n",
      "Number of all abstracts before pruning short articles = 25905\n",
      "Number after pruning short articles = 16511\n",
      "Number discarded for being too short: 9394\n",
      "Number of pruned articles:16511\n",
      "Wrote file:/home/krobasky/prompt/end2end/v1/data/csvpubs/pubmed23n1165.xml.gz.csv\n",
      "Converting file /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1166.xml.gz\n",
      "Number of all articles:10710\n",
      "Number of all abstracts before pruning short articles = 9250\n",
      "Number after pruning short articles = 5558\n",
      "Number discarded for being too short: 3692\n",
      "Number of pruned articles:5558\n",
      "Wrote file:/home/krobasky/prompt/end2end/v1/data/csvpubs/pubmed23n1166.xml.gz.csv\n",
      "Converting file /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1159.xml.gz\n",
      "Number of all articles:29998\n",
      "Number of all abstracts before pruning short articles = 25726\n",
      "Number after pruning short articles = 16430\n",
      "Number discarded for being too short: 9296\n",
      "Number of pruned articles:16430\n",
      "Wrote file:/home/krobasky/prompt/end2end/v1/data/csvpubs/pubmed23n1159.xml.gz.csv\n",
      "Converting file /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1161.xml.gz\n",
      "Number of all articles:29993\n",
      "Number of all abstracts before pruning short articles = 26017\n",
      "Number after pruning short articles = 15945\n",
      "Number discarded for being too short: 10072\n",
      "Number of pruned articles:15945\n",
      "Wrote file:/home/krobasky/prompt/end2end/v1/data/csvpubs/pubmed23n1161.xml.gz.csv\n",
      "Converting file /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1163.xml.gz\n",
      "Number of all articles:29998\n",
      "Number of all abstracts before pruning short articles = 22451\n",
      "Number after pruning short articles = 13904\n",
      "Number discarded for being too short: 8547\n",
      "Number of pruned articles:13904\n",
      "Wrote file:/home/krobasky/prompt/end2end/v1/data/csvpubs/pubmed23n1163.xml.gz.csv\n",
      "Converting file /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1157.xml.gz\n",
      "Number of all articles:29997\n",
      "Number of all abstracts before pruning short articles = 25160\n",
      "Number after pruning short articles = 15933\n",
      "Number discarded for being too short: 9227\n",
      "Number of pruned articles:15933\n",
      "Wrote file:/home/krobasky/prompt/end2end/v1/data/csvpubs/pubmed23n1157.xml.gz.csv\n",
      "Converting file /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1158.xml.gz\n",
      "Number of all articles:29991\n",
      "Number of all abstracts before pruning short articles = 27065\n",
      "Number after pruning short articles = 17844\n",
      "Number discarded for being too short: 9221\n",
      "Number of pruned articles:17844\n",
      "Wrote file:/home/krobasky/prompt/end2end/v1/data/csvpubs/pubmed23n1158.xml.gz.csv\n",
      "Converting file /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1162.xml.gz\n",
      "Number of all articles:29994\n",
      "Number of all abstracts before pruning short articles = 26858\n",
      "Number after pruning short articles = 17900\n",
      "Number discarded for being too short: 8958\n",
      "Number of pruned articles:17900\n",
      "Wrote file:/home/krobasky/prompt/end2end/v1/data/csvpubs/pubmed23n1162.xml.gz.csv\n",
      "Converting file /home/krobasky/prompt/end2end/v1/data/raw/pubs/pubmed23n1164.xml.gz\n",
      "Number of all articles:29986\n",
      "Number of all abstracts before pruning short articles = 26739\n",
      "Number after pruning short articles = 17326\n",
      "Number discarded for being too short: 9413\n",
      "Number of pruned articles:17326\n",
      "Wrote file:/home/krobasky/prompt/end2end/v1/data/csvpubs/pubmed23n1164.xml.gz.csv\n",
      "CPU times: user 2min 46s, sys: 3.27 s, total: 2min 49s\n",
      "Wall time: 3min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# takes about 3 minutes to do 10 files; or about 5 hours to do them all\n",
    "csv_list = create_pubcsv_dataset(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0788b0f-565e-4c3c-93ff-d110b78ca36f",
   "metadata": {},
   "source": [
    "# Create new CSVs that include GENES column\n",
    "use the search.awk script\n",
    "\n",
    "Fix the awk script to swallow the entire csv but only match on the title, abstract columns; then output the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "668907c0-9d9f-4061-bf73-1c6289dd9ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_gene_files(m: ReferenceData):\n",
    "    \"\"\" Calls the search.awk script in gpubs/scripts \"\"\"\n",
    "    \n",
    "    filtered_terms_file = os.path.join(m.search_path(), m.filtered_terms_filename)\n",
    "    csv_inpath = m.pub_outpath()\n",
    "    csv_outpath = m.genes_outpath()\n",
    "    verbose = m.verbose\n",
    "    \n",
    "    import glob\n",
    "    import subprocess\n",
    "    awk_script = \"gpubs/scripts/search.awk\"\n",
    "    for file_name_path in glob.glob(os.path.join(csv_inpath,\"pubmed*.xml.gz.csv\")):\n",
    "        file_name = os.path.basename(file_name_path)\n",
    "        input_csv_file = os.path.join(csv_inpath, file_name)\n",
    "        output_csv_file = os.path.join(csv_outpath, file_name)\n",
    "        msg2(verbose, f\"Creating {output_csv_file}\")\n",
    "        error_file = os.path.join(csv_outpath, f\"{file_name}.err\")\n",
    "        command = [awk_script, filtered_terms_file, input_csv_file]\n",
    "        with open(output_csv_file, \"w\") as output, open(error_file, \"w\") as error:\n",
    "            subprocess.run(command, stdout=output, stderr=error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f34db2f6-d5fb-4505-b97e-cbb915de7b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/krobasky/prompt/end2end/v1/data/csvpubs/genes/'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.genes_outpath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "95ed04bb-5aa0-406f-9328-7cd4e6625f25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating /home/krobasky/prompt/end2end/v1/data/csvpubs/genes/pubmed23n1162.xml.gz.csv\n",
      "Creating /home/krobasky/prompt/end2end/v1/data/csvpubs/genes/pubmed23n1161.xml.gz.csv\n",
      "Creating /home/krobasky/prompt/end2end/v1/data/csvpubs/genes/pubmed23n1158.xml.gz.csv\n",
      "Creating /home/krobasky/prompt/end2end/v1/data/csvpubs/genes/pubmed23n1165.xml.gz.csv\n",
      "Creating /home/krobasky/prompt/end2end/v1/data/csvpubs/genes/pubmed23n1160.xml.gz.csv\n",
      "Creating /home/krobasky/prompt/end2end/v1/data/csvpubs/genes/pubmed23n1157.xml.gz.csv\n",
      "Creating /home/krobasky/prompt/end2end/v1/data/csvpubs/genes/pubmed23n1164.xml.gz.csv\n",
      "Creating /home/krobasky/prompt/end2end/v1/data/csvpubs/genes/pubmed23n1166.xml.gz.csv\n",
      "Creating /home/krobasky/prompt/end2end/v1/data/csvpubs/genes/pubmed23n1159.xml.gz.csv\n",
      "Creating /home/krobasky/prompt/end2end/v1/data/csvpubs/genes/pubmed23n1163.xml.gz.csv\n",
      "CPU times: user 39.9 ms, sys: 214 ms, total: 254 ms\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this takes about 40s for 10 files, which is much slower than just running the awk script\n",
    "# also, we filter out about 42% of the abstracts, most of which are 2022\n",
    "# xxx ALSO - mask, flower, killer... these are all still in there, so the stop words aren't working, fix that.\n",
    "create_gene_files(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d099c875-7f97-45a0-9a4a-1e34a0f059bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91359\n",
      "    15947 ./v1/data/csvpubs/genes/pubmed23n1157.xml.gz.csv\n",
      "    17852 ./v1/data/csvpubs/genes/pubmed23n1158.xml.gz.csv\n",
      "    16430 ./v1/data/csvpubs/genes/pubmed23n1159.xml.gz.csv\n",
      "    18152 ./v1/data/csvpubs/genes/pubmed23n1160.xml.gz.csv\n",
      "    15946 ./v1/data/csvpubs/genes/pubmed23n1161.xml.gz.csv\n",
      "    17901 ./v1/data/csvpubs/genes/pubmed23n1162.xml.gz.csv\n",
      "    13904 ./v1/data/csvpubs/genes/pubmed23n1163.xml.gz.csv\n",
      "    17326 ./v1/data/csvpubs/genes/pubmed23n1164.xml.gz.csv\n",
      "    16528 ./v1/data/csvpubs/genes/pubmed23n1165.xml.gz.csv\n",
      "     5558 ./v1/data/csvpubs/genes/pubmed23n1166.xml.gz.csv\n",
      "   155544 total\n"
     ]
    }
   ],
   "source": [
    "!awk -F'\\t' '$10 != \"\"{print $10}' ./v1/data/csvpubs/genes/*.xml.gz.csv|wc -l\n",
    "! wc -l ./v1/data/csvpubs/genes/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "be45dbe2-9511-402e-8844-2ef5545ee11f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35616278\tRecent advances in anti-multidrug resistance for nano-drug delivery system.\tChemotherapy for tumors occasionally results in drug resistance, which is the major reason for the treatment failure. Higher drug doses could improve the therapeutic effect, but higher toxicity limits the further treatment. For overcoming drug resistance, functional nano-drug delivery system (NDDS) has been explored to sensitize the anticancer drugs and decrease its side effects, which are applied in combating multidrug resistance (MDR) via a variety of mechanisms including bypassing drug efflux, controlling drug release, and disturbing metabolism. This review starts with a brief report on the major MDR causes. Furthermore, we searched the papers from NDDS and introduced the recent advances in sensitizing the chemotherapeutic drugs against MDR tumors. Finally, we concluded that the NDDS was based on several mechanisms, and we looked forward to the future in this field.\tDrug delivery\t2022\tDrug delivery\tJournal Article\tAntineoplastic Agents,pharmacology,Drug Resistance, Multiple,Drug Resistance, Neoplasm,Humans,Nanoparticle Drug Delivery System,Nanoparticles,therapeutic use,Neoplasms,drug therapy,metabolism\tJournal Article,Review\n"
     ]
    }
   ],
   "source": [
    "#%%bash\n",
    "# This is SO much faster, but not as sustainable.\n",
    "#./gpubs/scripts/search.awk \\\n",
    "#  ./v4/data/search_terms/filtered_terms.txt \\\n",
    "#  ./v4/data/csvpubs/pubmed23n1166.xml.gz.csv \\\n",
    "#> ./v4/data/csvpubs/genes/pubmed23n1166.xml.gz.csv 2> ./v4/data/csvpubs/genes/pubmed23n1166.xml.gz.csv.err\n",
    "#\n",
    "# field 10 has the genes\n",
    "##bhead -1 ./v4/data/csvpubs/genes/pubmed23n1166.xml.gz.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c42fb2cb-0cea-47f4-be87-3dc8a2664264",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LD,MS,Rab,GO\n",
      "CIs,Cox,CI,HR,HRs\n",
      "MRSA,ERK,MEK\n",
      "Ga,NPs,LMs,CO,nm\n",
      "LOTUS\n",
      "cage\n",
      "tag,PS,MBP,PLD\n",
      "digit,tip\n",
      "nail\n",
      "FST\n",
      "MMR\n",
      "ASD,ID\n",
      "LRRK2\n",
      "TLR4,MSU,MyD88,TLR2\n",
      "HCC\n",
      "CAFs,T,killer\n",
      "MAT,PA\n",
      "GRS,PA,CR\n",
      "MRS,LCS\n",
      "PH\n",
      "lab,CH\n",
      "OT\n",
      "T\n",
      "MRI,CT\n",
      "MoS\n",
      "flower,SRC\n",
      "CS,GS\n",
      "grasp\n",
      "PhA\n",
      "polymerase\n",
      "spp,flower\n",
      "MINT\n",
      "pH\n",
      "rain\n",
      "P450,MES,bis\n",
      "mask\n",
      "Pb,Cr,P,pH,sand\n",
      "CD68\n",
      "HS\n",
      "T1,T2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat ./v1/data/csvpubs/genes/pubmed23n1166.xml.gz.csv.err\n",
    "awk -F'\\t' '$10 != \"\" {print $10}' ./v4/data/csvpubs/genes/pubmed23n1166.xml.gz.csv|head -120|tail -40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "655aaa88-f857-4f8b-9666-9ab460e6c100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LD,MS,Rab,GO\n",
      "CIs,Cox,CI,HR,HRs\n",
      "MRSA,ERK,MEK\n",
      "Ga,NPs,LMs,CO,nm\n",
      "LOTUS\n",
      "cage\n",
      "tag,PS,MBP,PLD\n",
      "digit,tip\n",
      "nail\n",
      "FST\n",
      "MMR\n",
      "ASD,ID\n",
      "LRRK2\n",
      "TLR4,MSU,MyD88,TLR2\n",
      "HCC\n",
      "CAFs,T,killer\n",
      "MAT,PA\n",
      "GRS,PA,CR\n",
      "MRS,LCS\n",
      "PH\n",
      "lab,CH\n",
      "OT\n",
      "T\n",
      "MRI,CT\n",
      "MoS\n",
      "flower,SRC\n",
      "CS,GS\n",
      "grasp\n",
      "PhA\n",
      "polymerase\n",
      "spp,flower\n",
      "MINT\n",
      "pH\n",
      "rain\n",
      "P450,MES,bis\n",
      "mask\n",
      "Pb,Cr,P,pH,sand\n",
      "CD68\n",
      "HS\n",
      "T1,T2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat ./v4/data/csvpubs/genes/pubmed23n1166.xml.gz.csv.err\n",
    "awk -F'\\t' '$10 != \"\" {print $10}' ./v4/data/csvpubs/genes/pubmed23n1166.xml.gz.csv|head -120|tail -40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "b284ffdc-a629-49c9-b823-e4d04bce01ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEM,PI\n",
      "CXCR4\n",
      "AKT,BPA\n",
      "SBS\n",
      "A7,A7\n",
      "PA,T,PI\n",
      "CG\n",
      "AFAP1-AS1,GRL\n",
      "CSD,PAAf,FAA\n",
      "Li,Li,SCL\n",
      "ACTRT1,SPACA1,ACTL7A,ACTRT1,ACTRT2,ARP,SPATA46,PARP11,PT,envelope,ACTL9,KO\n",
      "fate\n",
      "Pinch2,Rho,Cdc42,IPP,hub,Pinch1,Ilk,Pinch2,RhoA\n",
      "PSM,OC\n",
      "Coma,DBS,GCS\n",
      "clock,NAMPT,CLOCK,BMAL1,fat\n",
      "E1,fat,GIP,E1,DO,fat\n",
      "MALAT1,HDAC4,MALAT1,polymerase\n",
      "CG,EG\n",
      "NP\n",
      "II\n",
      "iNOS,eNOS,sGC\n",
      "ES,HPO\n",
      "IL-15,IL-15\n",
      "ANOVA\n",
      "MS,MT,RT\n",
      "BRAF\n",
      "FXR,YAP1,HCC,FXR,Yap1,YAP1\n",
      "CO\n",
      "POC,POC,Delta\n",
      "km\n",
      "EpCAM\n",
      "OCT\n",
      "AMPs,C-C,Fish,CCL28\n",
      "H,Gb5\n",
      "LHb\n",
      "PHEs\n",
      "PPCS,trio,PPCS\n",
      "BPA,post\n",
      "osf\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "awk -F'\\t' '$10 != \"\" {print $10}' ./v4/data/csvpubs/genes/pubmed23n1166.xml.gz.csv|head -40\n",
    "#awk -F'\\t' '$10 != \"\" {print $2\"\\n\"$3\"\\n\"$10,\"\\n\"}' ./v4/data/csvpubs/genes/pubmed23n1166.xml.gz.csv|head -40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a0ca2-e72a-405b-b0ef-6cc158126da7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Wrap the pipeline\n",
    "\n",
    "The pipeline should be all callable from python so you can use the model. \n",
    "\n",
    "wrap those bash scripts into python api's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a527e-a5b8-4528-bd41-1c674d69fc19",
   "metadata": {},
   "source": [
    "# DONE: Package code\n",
    "\n",
    "make a setup.py, conda.yml, etc.\n",
    "\n",
    "put in github under krobasky: repo \"gpubs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6858b261-0bd3-41bc-81f8-341ee5a6dcac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578bc623-606b-43bf-8192-a5d7ca149ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e943489-dee5-4d6f-afc6-dbbe46207c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6808ec-9fa3-4ee8-a18d-78798718dee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caaa3f7-0d9e-4eaa-a198-97ef3a7b919a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f9b5e-f8c9-4d4a-9476-c1820fe747ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3548c92-981c-4f11-94f6-4c7958be670b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created v1/data/search_terms/search_terms.txt.\n",
      "Created v1/data/search_terms/search_terms.txt.unsorted - can be removed.\n",
      "Number of lines in v1/data/search_terms/search_terms.txt: 338143\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "VERSION_ROOT=\"v1/data\"\n",
    "VERBOSE=2\n",
    "\n",
    "./gpubs/scripts/create_search_terms_file.sh --root-dir $VERSION_ROOT \\\n",
    "   --ref-dir \"reference\" \\\n",
    "   --dbx-dir \"dbxrefs\" \\\n",
    "   --dbx \"AllianceGenome.txt Ensembl.txt HGNC.txt IMGT_GENE-DB.txt\" \\\n",
    "   --search-dir \"search_terms\" \\\n",
    "   --search-file \"search_terms.txt\" \\\n",
    "   --gene-symbols \"gene_symbols.txt\" \\\n",
    "   --gene-synonyms \"gene_synonyms.txt\" \\\n",
    "   --verbose $VERBOSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prompt]",
   "language": "python",
   "name": "conda-env-prompt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
