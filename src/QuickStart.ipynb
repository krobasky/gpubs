{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bc2f3b4-b195-45df-b1df-5ec1b20da9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_gene_symbols(output_filepath = \"data/raw/gene_info.gz\", url = \"https://ftp.ncbi.nlm.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz\", verbose=0):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(output_filepath, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    msginfo(verbose, \"Download completed.\")\n",
    "\n",
    "def extract_gene_data(filepath = \"data/raw/gene_info.gz\"):\n",
    "    \"\"\" returns 3 sets \"\"\"\n",
    "    import gzip\n",
    "\n",
    "    gene_symbols = set()\n",
    "    dbxrefs = set()\n",
    "    gene_synonyms = set()\n",
    "\n",
    "    with gzip.open(filepath, \"rt\") as file:\n",
    "        for line in file:\n",
    "            if not line.startswith(\"#\"):\n",
    "                fields = line.strip().split(\"\\t\")\n",
    "                gene_symbols.add(fields[2])\n",
    "\n",
    "                # Extract dbXrefs\n",
    "                dbxref_field = fields[5]\n",
    "                if dbxref_field != \"-\":\n",
    "                    identifiers = dbxref_field.split(\"|\")\n",
    "                    dbxrefs.update(identifiers)\n",
    "\n",
    "                # Extract gene synonyms\n",
    "                synonym_field = fields[4]\n",
    "                if synonym_field != \"-\":\n",
    "                    synonyms = synonym_field.split(\"|\")\n",
    "                    gene_synonyms.update(synonyms)\n",
    "\n",
    "    return gene_symbols, dbxrefs, gene_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94eec2e-f95c-4f95-9ad6-a0c05868c0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_search_stop(search_file, stop_words):\n",
    "    # Load search terms from the file into a set\n",
    "    search_terms=[]\n",
    "    with open(search_file, 'r') as f:\n",
    "        search_terms = [line.strip() for line in f]\n",
    "    return search_terms\n",
    "\n",
    "def filter_search_terms(search_terms, stop_words):\n",
    "    filtered_terms = []\n",
    "    matched_stop_words = []\n",
    "    \n",
    "    # get the matched stop words and keep in original case\n",
    "    for term in search_terms:\n",
    "        if term.lower() in stop_words:\n",
    "            matched_stop_words.append(term)\n",
    "\n",
    "    # get the filtered terms, lower case them\n",
    "    filtered_terms = [term.lower() for term in search_terms if term.lower() not in stop_words]\n",
    "\n",
    "    # add the two lists together so the matched terms retain original case\n",
    "    # this will allow for finding genes that are also common english words\n",
    "    final_terms = filtered_terms + matched_stop_words\n",
    "    return (final_terms, filtered_terms, matched_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ace36021-359a-4494-84ac-bb48109a68fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import string\n",
    "\n",
    "def fetch_brown_corpus():\n",
    "    from nltk.corpus import brown\n",
    "    corpus = 'brown'\n",
    "    # Select a specific category from the Brown Corpus for analysis\n",
    "    category = 'learned'\n",
    "    # Load the Brown Corpus\n",
    "    nltk.download('brown')\n",
    "    # Get the words from the selected category\n",
    "    words = brown.words(categories=category)\n",
    "    return words\n",
    "\n",
    "def read_search_stop(search_file, stop_words):\n",
    "    # Load search terms from the file into a set\n",
    "    search_terms=[]\n",
    "    with open(search_file, 'r') as f:\n",
    "        search_terms = [line.strip() for line in f]\n",
    "    return search_terms\n",
    "\n",
    "def filter_search_terms(search_terms, stop_words):\n",
    "    filtered_terms = []\n",
    "    matched_stop_words = []\n",
    "    \n",
    "    # get the matched stop words and keep in original case\n",
    "    for term in search_terms:\n",
    "        if term.lower() in stop_words:\n",
    "            matched_stop_words.append(term)\n",
    "\n",
    "    # get the filtered terms, lower case them\n",
    "    filtered_terms = [term.lower() for term in search_terms if term.lower() not in stop_words]\n",
    "\n",
    "    # add the two lists together so the matched terms retain original case\n",
    "    # this will allow for finding genes that are also common english words\n",
    "    final_terms = filtered_terms + matched_stop_words\n",
    "    return (final_terms, filtered_terms, matched_stop_words)\n",
    "\n",
    "def create_stop_words(frequency_list_outpath, custom_words) -> Set:\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    # Download the stopwords corpus\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    # Load the existing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Read the words from the frequency_list.txt file\n",
    "    with open(frequency_list_outpath, 'r') as file:\n",
    "        frequency_words = file.read().splitlines()\n",
    "\n",
    "    # Add the frequency words to the stop words set\n",
    "    stop_words.update(frequency_words)\n",
    "\n",
    "    # Add custom words\n",
    "    stop_words.update(custom_words)\n",
    "    return stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1eb2b544-a7de-4452-bb23-8047f67fbe6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import urllib.request\n",
    "from urllib.error import URLError\n",
    "from shutil import rmtree\n",
    "\n",
    "def check_disk_space(predicted_size, download_dir, verbose):\n",
    "    required_space = predicted_size\n",
    "    available_space = os.statvfs(download_dir).f_frsize * os.statvfs(download_dir).f_bavail\n",
    "    required_space_human = subprocess.check_output(['numfmt', '--to=iec-i', '--suffix=B', str(required_space)]).decode().strip()\n",
    "    available_space_human = subprocess.check_output(['numfmt', '--to=iec-i', '--suffix=B', str(available_space)]).decode().strip()\n",
    "\n",
    "    msg1(verbose, f\"Predicted download size = {required_space_human}, Available space = {available_space_human}\")\n",
    "\n",
    "    if required_space > available_space:\n",
    "        print(f\"Insufficient disk space! Required: {required_space_human}, Available: {available_space_human}\")\n",
    "        exit(1)\n",
    "\n",
    "def download_file(url, file_path, verbose):\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "    except URLError as e:\n",
    "        msg1(verbose, f\"Error downloading file: {url}\")\n",
    "        msg1(verbose, f\"Reason: {str(e.reason)}\")\n",
    "        #exit(1)\n",
    "\n",
    "def verify_md5(file_path, md5_file_path, verbose):\n",
    "    try:\n",
    "        output = subprocess.check_output(['md5sum', '-c', os.path.basename(md5_file_path)], cwd=os.path.dirname(md5_file_path), stderr=subprocess.DEVNULL).decode()\n",
    "        #output = subprocess.check_output(['md5sum', '-c', md5_file_path], stderr=subprocess.DEVNULL).decode()\n",
    "        if \"OK\" in output:\n",
    "            msg2(verbose, f\"{md5_file_path}: OK - MD5 checksum verification succeeded.\")\n",
    "        else:\n",
    "            msg1(verbose, f\"ERROR: {md5_file_path}: FAILED - MD5 checksum verification failed.\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        msg1(verbose, f\"ERROR: {md5_file_path}: FAILED - MD5 checksum verification failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b8b228e-ae5c-467a-b2e5-e1c1af94c708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def extract_data(element):\n",
    "    data = {}\n",
    "    data['PMID'] = element.findtext('MedlineCitation/PMID')\n",
    "    data['Title'] = element.findtext('MedlineCitation/Article/ArticleTitle')\n",
    "    data['Abstract'] = element.findtext('MedlineCitation/Article/Abstract/AbstractText')\n",
    "    data['Journal'] = element.findtext('MedlineCitation/Article/Journal/Title')\n",
    "    data['PublicationDate'] = element.findtext('MedlineCitation/Article/Journal/JournalIssue/PubDate/Year')\n",
    "    data['JournalTitle'] = element.findtext('MedlineCitation/Article/Journal/Title')\n",
    "    data['ArticleType'] = element.findtext('MedlineCitation/Article/PublicationTypeList/PublicationType')\n",
    "    \n",
    "    # Extract the descriptor names and qualifier names from the XML\n",
    "    mesh_headings = element.findall('.//MeshHeading')\n",
    "    mesh_heading_list = []\n",
    "    for heading in mesh_headings:\n",
    "        descriptor_name = heading.findtext('DescriptorName')\n",
    "        qualifier_names = [qualifier.text for qualifier in heading.findall('QualifierName')]\n",
    "        mesh_heading_list.append(descriptor_name)\n",
    "        mesh_heading_list.extend(qualifier_names)\n",
    "    data['MeshHeadingList'] = ','.join(mesh_heading_list)\n",
    "                                        \n",
    "    publication_types = element.findall('MedlineCitation/Article/PublicationTypeList/PublicationType')\n",
    "    data['PublicationTypeList'] = \",\".join([ptype.text for ptype in publication_types])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def prune_df(df, length_threshold = 405, verbose=2):\n",
    "    # exclude articles with no abstract, no date, or abstracts that are too short (less than length_threshold letters)\n",
    "    pruned_df = df[df['Abstract'].notna() & df['PublicationDate'].notna()]\n",
    "\n",
    "    # cut out any short articles\n",
    "    all_pruned = len(pruned_df)\n",
    "    msg2(verbose, f\"Number of all abstracts before pruning short articles = {all_pruned}\")\n",
    "    pruned_df = pruned_df[pruned_df['Abstract'].str.len() >= length_threshold]\n",
    "    long_pruned = len(pruned_df)\n",
    "    msg2(verbose, f\"Number after pruning short articles = {long_pruned}\")\n",
    "    msg2(verbose, f\"Number discarded for being too short: {all_pruned - long_pruned}\")\n",
    "\n",
    "    return pruned_df\n",
    "\n",
    "def get_pub_df(filename, inpath, outpath, length_threshold, prune=True,verbose=0):\n",
    "    import gzip\n",
    "    import xml.etree.ElementTree as ET\n",
    "    import pandas as pd\n",
    "\n",
    "    pubmed_filepath = os.path.join(inpath, filename)\n",
    "    # Open the gzip'd XML file\n",
    "    with gzip.open(pubmed_filepath, 'rb') as f:\n",
    "        # Read the contents of the gzip'd file\n",
    "        gzip_content = f.read()\n",
    "\n",
    "    # Parse the XML content using ElementTree\n",
    "    root = ET.fromstring(gzip_content)\n",
    "\n",
    "    # Extract data from each article and store in a list\n",
    "    articles = []\n",
    "    for article in root.findall('.//PubmedArticle'):\n",
    "        articles.append(extract_data(article))\n",
    "\n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(articles)\n",
    "    df = df.drop_duplicates()\n",
    "    if prune:\n",
    "        msg2(verbose, f\"Number of all articles:{len(df)}\")\n",
    "        df = prune_df(df, length_threshold = length_threshold, verbose=verbose)\n",
    "        msg2(verbose, f\"Number of pruned articles:{len(df)}\")\n",
    "    \n",
    "    # convert objects to simple types\n",
    "    df['PublicationDate'] = df['PublicationDate'].astype(int)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1959fdd2-3123-4f8f-9da2-2a572217342c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_gene_reference_data(m: ReferenceData):\n",
    "    \n",
    "    raw_gene_info_filepath = os.path.join(m.raw_path(), m.gene_info_filename)\n",
    "    reference_gene_symbols_filepath = os.path.join(m.reference_path(), m.gene_symbols_filename)\n",
    "    reference_gene_synonyms_filepath = os.path.join(m.reference_path(), m.gene_synonyms_filename)\n",
    "    dbxref_path = m.dbxref_path()\n",
    "    verbose = m.verbose\n",
    "    url = m.ncbi_gene_info_url\n",
    "    \n",
    "    # Download the gene symbols file\n",
    "    download_gene_symbols(output_filepath = raw_gene_info_filepath, url = url, verbose = verbose)\n",
    "\n",
    "    # Extract gene data\n",
    "    gene_symbols, dbxrefs, gene_synonyms = extract_gene_data(filepath = raw_gene_info_filepath)\n",
    "\n",
    "    # Save gene symbols to a file\n",
    "    with open(reference_gene_symbols_filepath, \"w\") as file:\n",
    "        for symbol in gene_symbols:\n",
    "            file.write(symbol + \"\\n\")\n",
    "\n",
    "    msg2(verbose, f\"Gene symbols saved to {reference_gene_symbols_filepath}\")\n",
    "\n",
    "    # Save dbXrefs to separate files\n",
    "    for identifier in dbxrefs:\n",
    "            identifier_parts = identifier.split(\":\")\n",
    "            identifier_type = identifier_parts[0].replace('/','_')\n",
    "            identifier_value = \":\".join(identifier_parts[1:])\n",
    "            filename = f\"{dbxref_path}/{identifier_type}.txt\"\n",
    "            with open(filename, \"a\") as file:\n",
    "                file.write(identifier_value + \"\\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    for identifiers in dbxrefs:\n",
    "        for identifier in identifiers:\n",
    "            identifier_parts = identifier.split(\":\")\n",
    "            identifier_type = identifier_parts[0].replace('/','_')\n",
    "            identifier_value = \":\".join(identifier_parts[1:])\n",
    "            filename = f\"{dbxref_path}/{identifier_type}.txt\"\n",
    "            with open(filename, \"a\") as file:\n",
    "                file.write(identifier_value + \"\\n\")\n",
    "    \"\"\"\n",
    "    msg2(verbose, \"dbXrefs saved to individual files.\")\n",
    "\n",
    "    # Save gene synonyms to a file\n",
    "    with open(reference_gene_synonyms_filepath, \"w\") as file:\n",
    "        for synonym in gene_synonyms:\n",
    "            file.write(synonym + \"\\n\")\n",
    "\n",
    "    msg2(verbose, f\"Gene synonyms saved to {reference_gene_synonyms_filepath}\")\n",
    "\n",
    "def create_frequency_list(m: ReferenceData) -> List:\n",
    "    \n",
    "    frequency_list_outpath = os.path.join(m.search_path(), m.frequency_list_filename)\n",
    "    stop_word_list_length = m.corpus_stop_word_list_length\n",
    "    verbose = m.verbose\n",
    "    \n",
    "    import nltk\n",
    "    from nltk import FreqDist\n",
    "    import string\n",
    "\n",
    "    words = fetch_brown_corpus()\n",
    "\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = [word.lower() for word in words if word not in string.punctuation and word.isalnum()]\n",
    "\n",
    "    # Compute the frequency distribution of words\n",
    "    freq_dist = FreqDist(words)\n",
    "\n",
    "    # Get the most frequent words\n",
    "    most_common_words = freq_dist.most_common(stop_word_list_length)\n",
    "\n",
    "    # Write the frequency list to a file\n",
    "    with open(frequency_list_outpath, 'w') as file:\n",
    "        for word, frequency in most_common_words:\n",
    "            file.write(word + '\\n')\n",
    "    msg2(verbose, f\"Wrote {frequency_list_outpath}\")\n",
    "    return most_common_words\n",
    "\n",
    "def create_search_terms_file(m: ReferenceData):\n",
    "    import os\n",
    "\n",
    "    dbxrefs = m.dbxrefs\n",
    "    dbxrefs_path = m.dbxref_path()  \n",
    "    gene_symbols_filepath = os.path.join(m.reference_path(), m.gene_symbols_filename)\n",
    "    gene_synonyms_filepath = os.path.join(m.reference_path(), m.gene_synonyms_filename)\n",
    "    search_terms_filepath = os.path.join(m.search_path(), m.search_terms_filename)\n",
    "    verbose = m.verbose\n",
    "\n",
    "    if dbxrefs == []:\n",
    "        # Get a list of all files in the directory\n",
    "        dbxrefs = os.listdir(dbxrefs_path)\n",
    "        # Filter out directories from the list\n",
    "        dbxrefs = [f for f in dbxrefs if os.path.isfile(os.path.join(dbxrefs_path, f))]\n",
    "  \n",
    "    \n",
    "    with open(f\"{search_terms_filepath}.unsorted\", \"w\") as outfile:\n",
    "        for ref in dbxrefs:\n",
    "            with open(os.path.join(dbxrefs_path, ref)) as infile:\n",
    "                sorted_lines = sorted(set(infile.readlines()))\n",
    "                outfile.writelines(sorted_lines)\n",
    "                #outfile.write(infile.read())\n",
    "\n",
    "        with open(gene_symbols_filepath) as infile:\n",
    "            sorted_lines = sorted(set(infile.readlines()))\n",
    "            outfile.writelines(sorted_lines)\n",
    "            #outfile.write(infile.read())\n",
    "\n",
    "        with open(gene_synonyms_filepath) as infile:\n",
    "            sorted_lines = sorted(set(infile.readlines()))\n",
    "            outfile.writelines(sorted_lines)\n",
    "            #outfile.write(infile.read())\n",
    "\n",
    "    # Sort and remove duplicates from the search terms file\n",
    "    search_terms_unsorted_filepath = f\"{search_terms_filepath}.unsorted\"\n",
    "    os.system(f\"sort -u {search_terms_unsorted_filepath} | grep -v not > {search_terms_filepath}\")\n",
    "\n",
    "    msg2(verbose, f\"Created {search_terms_filepath}.\")\n",
    "    msg2(verbose, f\"Created {search_terms_unsorted_filepath} - can be removed.\")\n",
    "    line_count = sum(1 for line in open(search_terms_filepath))\n",
    "    msg2(verbose, f\"Number of lines in {search_terms_filepath}: {line_count}\")\n",
    "\n",
    "def create_filtered_search_terms(m: ReferenceData) -> List:\n",
    "    \n",
    "    search_file = os.path.join(m.search_path(), m.search_terms_filename)\n",
    "    frequency_list_outpath = os.path.join(m.search_path(), m.frequency_list_filename)\n",
    "    custom_words = m.custom_stop_words\n",
    "    final_file = os.path.join(m.search_path(), m.filtered_terms_filename)\n",
    "    verbose = m.verbose\n",
    "    \n",
    "    stop_words = create_stop_words(frequency_list_outpath, custom_words)\n",
    "    \n",
    "    search_terms = read_search_stop(search_file = search_file, stop_words = stop_words)\n",
    "    msg2(verbose, f\"Number of original search_terms:{len(search_terms)}\")\n",
    "    final_terms, filtered_terms, matched_stop_words = filter_search_terms(search_terms, stop_words)\n",
    "    msg2(verbose, f\"number of filtered_terms:{len(filtered_terms)}\\nfinal number of final_terms:{len(final_terms)}\\n number of matched_stop_words:{len(matched_stop_words)}\\nmatched_stop_words={matched_stop_words}\")\n",
    "    if final_file is not None:\n",
    "        with open(final_file, \"w\") as f:\n",
    "            f.writelines('\\n'.join(final_terms))\n",
    "    msg2(verbose, f\"Created {final_file}\")\n",
    "    return final_terms\n",
    "\n",
    "\n",
    "def fetch_abstracts(m: ReferenceData):\n",
    "    \n",
    "    num_files = m.num_abstract_xml_files\n",
    "    refresh = m.refresh_abstract_xml_files\n",
    "    download_dir = m.pub_inpath()\n",
    "    verbose = m.verbose\n",
    "    \n",
    "    \"\"\" This can probably be done faster with download_files.sh \"\"\" \n",
    "    msg2(verbose, f\"Download Directory: {download_dir}\")\n",
    "    msg2(verbose, f\"Number of abstracts to ensure have been downloaded: {num_files}\")\n",
    "    msg2(verbose, f\"Refresh: {refresh}\")\n",
    "\n",
    "    # FTP settings\n",
    "    ftp_host = \"ftp.ncbi.nlm.nih.gov\"\n",
    "    ftp_path = \"/pubmed/baseline/\"\n",
    "\n",
    "    # Retrieve file names and find the largest number\n",
    "    #file_list = subprocess.check_output(['curl', '-s', f\"ftp://{ftp_host}{ftp_path}\"]).decode().splitlines()\n",
    "    \n",
    "    output = subprocess.check_output(['curl', '-s', f\"ftp://{ftp_host}{ftp_path}\"]).decode()\n",
    "    file_list = [line.split()[-1] for line in output.splitlines() if line.endswith(\".xml.gz\")]\n",
    "\n",
    "    msg2(verbose, f\"Total number of NCBI abstract XML files: {len(file_list)}\")\n",
    "    latest_files = [file_name for file_name in file_list if file_name.startswith(\"pubmed23n\") and file_name.endswith(\".xml.gz\")]\n",
    "    latest_files.sort(reverse=True)\n",
    "    latest_files = latest_files[:num_files]\n",
    "    msg2(verbose, f\"latest_files {num_files}: {latest_files}\")\n",
    "\n",
    "    # Check if enough files are available\n",
    "    if len(latest_files) == 0:\n",
    "        msg1(verbose, \"Error: Insufficient number of files available!\")\n",
    "        exit(1)\n",
    "\n",
    "    # Calculate total predicted size\n",
    "    total_size = 0\n",
    "    for file_name in latest_files:\n",
    "        response = subprocess.check_output(['curl', '-sI', f\"ftp://{ftp_host}{ftp_path}{file_name}\"]).decode()\n",
    "        file_size = int(response.split(\"Content-Length: \")[1].split(\"\\r\")[0])\n",
    "        total_size += file_size\n",
    "\n",
    "    # Check disk space before downloading\n",
    "    check_disk_space(total_size, download_dir, verbose=verbose)\n",
    "\n",
    "    # Download and check files\n",
    "    for file_name in latest_files:\n",
    "        md5_file_name = f\"{file_name}.md5\"\n",
    "        file_path = os.path.join(download_dir, file_name)\n",
    "        md5_file_path = os.path.join(download_dir, md5_file_name)\n",
    "\n",
    "        # Refresh files that were previously downloaded?\n",
    "        if not refresh:\n",
    "            # No, so skip downloading those again\n",
    "\n",
    "            # If one file or the other is missing, you still have to do a download\n",
    "            # Here, just provide information as to which files are present.\n",
    "            if os.path.isfile(file_path) and not os.path.isfile(md5_file_path):\n",
    "                msg1(verbose, f\"ERROR: Missing - {md5_file_path}; re-downloading now\")\n",
    "            if not os.path.isfile(file_path) and os.path.isfile(md5_file_path):\n",
    "                msg1(verbose, f\"ERROR: Missing - {file_path}; re-downloading now\")\n",
    "\n",
    "            if os.path.isfile(file_path) and os.path.isfile(md5_file_path):\n",
    "                msg1(verbose, f\"SKIP: {file_path} exists.\")\n",
    "                continue\n",
    "\n",
    "        # Check file size\n",
    "        response = subprocess.check_output(['curl', '-sI', f\"ftp://{ftp_host}{ftp_path}{file_name}\"]).decode()\n",
    "        file_size = int(response.split(\"Content-Length: \")[1].split(\"\\r\")[0])\n",
    "\n",
    "        msg2(verbose, f\"File: {file_name}, Size: {file_size} bytes\")\n",
    "\n",
    "        # Download file\n",
    "        msg2(verbose, f\"WARNING: Downloading: {file_name} to {download_dir}\")\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        download_file(f\"ftp://{ftp_host}{ftp_path}{file_name}\", file_path, verbose)\n",
    "\n",
    "        # Download MD5 file\n",
    "        if os.path.isfile(md5_file_path):\n",
    "            os.remove(md5_file_path)\n",
    "        download_file(f\"ftp://{ftp_host}{ftp_path}{md5_file_name}\", md5_file_path, verbose)\n",
    "\n",
    "        # Check MD5\n",
    "        verify_md5(file_path, md5_file_path, verbose)\n",
    "\n",
    "    total_size_human = subprocess.check_output(['numfmt', '--to=iec-i', '--suffix=B', str(total_size)]).decode().strip()\n",
    "    msg2(verbose, f\"Total size of abstract files: {total_size_human}\")\n",
    "\n",
    "def create_pubcsv_dataset(m: ReferenceData) -> List:\n",
    "    \"\"\" Takes about 14min for 30 (2 per minute) \"\"\"\n",
    "    \n",
    "    abstract_length_threshold = m.abstract_length_threshold\n",
    "    pub_inpath = m.pub_inpath()\n",
    "    pub_outpath = m.pub_outpath()\n",
    "    verbose = m.verbose\n",
    "\n",
    "    import os\n",
    "    import glob\n",
    "    \n",
    "    csv_list = []\n",
    "    # Iterate through files in the directory\n",
    "    for filepath in glob.glob(os.path.join(pub_inpath, \"pubmed*.xml.gz\")):\n",
    "        msg2(verbose, f\"Converting file {filepath}\")\n",
    "        if os.path.isfile(filepath):\n",
    "            filename = os.path.basename(filepath)\n",
    "            df = get_pub_df(filename=filename, inpath=pub_inpath, outpath= pub_outpath, prune=True, length_threshold = abstract_length_threshold, verbose = verbose)\n",
    "            csv_filepath = os.path.join(pub_outpath, f\"{filename}.csv\")\n",
    "            df.to_csv(csv_filepath, header=False, index=False, sep=\"\\t\")\n",
    "            msg2(verbose, f\"Wrote file:{csv_filepath}\")\n",
    "            csv_list.append(csv_filepath)\n",
    "            \n",
    "    return(csv_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e578d05-f2f9-44ff-b637-29f75d64cf30",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f97e6-cd31-4ade-9279-a301ccfd6e8a",
   "metadata": {},
   "source": [
    "## Reload libraries in case the changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bbbf65d8-6440-4077-8f98-ad360ed597d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "import gpubs\n",
    "from gpubs.models import ReferenceData\n",
    "from gpubs.api import create_gene_reference_data, create_frequency_list, create_search_terms_file, create_filtered_search_terms, fetch_abstracts, create_pubcsv_dataset, create_gene_files\n",
    "\n",
    "import importlib\n",
    "importlib.reload(gpubs)\n",
    "importlib.reload(gpubs.models)\n",
    "importlib.reload(gpubs.api)\n",
    "\n",
    "import gpubs\n",
    "from gpubs.models import ReferenceData\n",
    "from gpubs.api import create_gene_reference_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beac480-e526-4a37-a758-7eb23773b115",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "095a3b77-efa5-412e-b346-94ec51164085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version_root=/home/krobasky/prompt/repo/gpubs/src/../../v1/data/\n",
      "Created directory structure.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ReferenceData(ncbi_gene_info_url='https://ftp.ncbi.nlm.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz', data_root='data/', raw_data_path='raw/', reference_data_path='reference/', dbxref_reference_data_path='dbxrefs/', dbxrefs=['AllianceGenome.txt', 'Ensembl.txt', 'HGNC.txt', 'IMGT_GENE-DB.txt'], gene_info_filename='gene_info.gz', gene_symbols_filename='gene_symbols.txt', gene_synonyms_filename='gene_synonyms.txt', search_terms_path='search_terms/', frequency_list_filename='frequency_list.txt', corpus_stop_word_list_length=4000, custom_stop_words=['ago', 'aim', 'amid', 'april', 'arch', 'bed', 'bite', 'bug', 'cage', 'co', 'crop', 'damage', 'danger', 'digit', 'et', 'fast', 'fat', 'fate', 'fire', 'flower', 'gap', 'genesis', 'gov', 'gpa', 'grasp', 'ii', 'inos', 'iv', 'killer', 'lab', 'lamp', 'laser', 'map', 'mask', 'mater', 'melt', 'mice', 'minor', 'miss', 'mv', 'nail', 'net', 'not', 'osf', 'pan', 'par', 'pha', 'rab', 'race', 'rain', 'rank', 'san', 'sand', 'se', 'sink', 'soft', 'spatial', 'spin', 'spp', 'steel', 'stop', 'storm', 'tactile', 'tau', 'theta', 'tip', 'traits', 'via'], search_terms_filename='search_terms.txt', filtered_terms_filename='filtered_terms.txt', abstract_inpath='pubs/', refresh_abstract_xml_files=False, num_abstract_xml_files=5, abstract_outpath='csvpubs/', abstract_length_threshold=405, abstract_genes_outpath='genes/', verbose=2, debug=False, version='../../v1', rand_seed=42, version_root='/home/krobasky/prompt/repo/gpubs/src/../../v1/data/')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data model\n",
    "m = ReferenceData(version = \"../../v1\",       # make data root above any git repo\n",
    "                  verbose = 2,                # print all the info messages\n",
    "                  num_abstract_xml_files = 5, # only fetch 5 files from NCBI\n",
    "                  dbxrefs = [\"AllianceGenome.txt\", \"Ensembl.txt\", \"HGNC.txt\", \"IMGT_GENE-DB.txt\"]  # exclude miRNA and MIM\n",
    "\n",
    "                 )\n",
    "\n",
    "# check the modelvalues\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3a8c2aec-e181-4cd4-80aa-61e077eeb485",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed.\n",
      "Gene symbols saved to /home/krobasky/prompt/repo/gpubs/src/../../v1/data/reference/gene_symbols.txt\n",
      "dbXrefs saved to individual files.\n",
      "Gene synonyms saved to /home/krobasky/prompt/repo/gpubs/src/../../v1/data/reference/gene_synonyms.txt\n"
     ]
    }
   ],
   "source": [
    "# Fetch data/raw/gene_info.gz and create the human genes lists under data/reference (gene_symbols.txt, gene_synonyms.txt, dbxrefs/*)\n",
    "create_gene_reference_data(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82e368ba-d4b3-4356-b58e-42cf0d7c049b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/krobasky/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /home/krobasky/prompt/repo/gpubs/src/../../v1/data/search_terms/frequency_list.txt\n"
     ]
    }
   ],
   "source": [
    "# The goal of the following 3 calls is to \n",
    "# create data/search_terms/filtered_terms.txt from english language corpus\n",
    "\n",
    "# Create a word frequency list from an English language corpus\n",
    "_ = create_frequency_list(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ddd7382a-8e3f-421d-849d-a7c908d215c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /home/krobasky/prompt/repo/gpubs/src/../../v1/data/search_terms/search_terms.txt.\n",
      "Created /home/krobasky/prompt/repo/gpubs/src/../../v1/data/search_terms/search_terms.txt.unsorted - can be removed.\n",
      "Number of lines in /home/krobasky/prompt/repo/gpubs/src/../../v1/data/search_terms/search_terms.txt: 338143\n"
     ]
    }
   ],
   "source": [
    "# Create the file of gene search terms (data/search_terms/search_terms.txt) using stop words from frequency list\n",
    "create_search_terms_file(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "561e971d-2029-4037-905f-a71bcc893ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original search_terms:338143\n",
      "number of filtered_terms:337951\n",
      "final number of final_terms:338143\n",
      " number of matched_stop_words:192\n",
      "matched_stop_words=['ABO', 'ACE', 'ACT', 'AF', 'AGO', 'AID', 'AIM', 'AIR', 'ALL', 'AM', 'AMID', 'AN', 'APRIL', 'APT', 'ARC', 'ARCH', 'ARM', 'ARMS', 'ART', 'AS', 'ASK', 'AT', 'BAD', 'BANK', 'BASE', 'BED', 'BEST', 'BITE', 'BOD', 'BORIS', 'BRIGHT', 'BUG', 'CAGE', 'CALL', 'CAN', 'CAR', 'CAT', 'CELL', 'CHIP', 'CO', 'CROP', 'DAMAGE', 'DANGER', 'DC', 'DIGIT', 'DO', 'END', 'ET', 'ETA', 'FACE', 'FACT', 'FAST', 'FAT', 'FATE', 'FIND', 'FIRE', 'FLOWER', 'FOR', 'GAP', 'GAS', 'Genesis', 'GET', 'GO', 'GOV', 'GPA', 'GRASP', 'GREAT', 'H', 'HAD', 'HAS', 'HE', 'hELD', 'HIS', 'hole', 'HOT', 'HR', 'iCE', 'ICE', 'IF', 'II', 'IMPACT', 'IN', 'INOS', 'IV', 'JET', 'KILLER', 'LAB', 'LAMP', 'LARGE', 'LASER', 'LED', 'LIGHT', 'LIME', 'LIMIT', 'MA', 'MAIL', 'MAP', 'MARCH', 'MARK', 'MARS', 'MASK', 'MASS', 'MATER', 'ME', 'MELT', 'MEN', 'Met', 'MET', 'MG', 'MICE', 'MINOR', 'MISS', 'ML', 'MV', 'NAIL', 'NEST', 'NET', 'NOT', 'NS', 'ODD', 'OF', 'ON', 'OSF', 'OUT', 'P', 'PACE', 'PAINT', 'PAN', 'PAR', 'PARTICLE', 'PAST', 'PBS', 'PER', 'PERMIT', 'PH', 'PHA', 'PILOT', 'PIP', 'PLANE', 'POEM', 'POST', 'RAB', 'RACE', 'RAIN', 'RANK', 'RED', 'REST', 'RH', 'SAN', 'SAND', 'SE', 'SECRET', 'SERA', 'SET', 'SHARP', 'SHE', 'SHOT', 'SIMPLE', 'SINK', 'SOFT', 'SPATIAL', 'SPIN', 'SPOT', 'SPP', 'SPRING', 'STEEL', 'STEP', 'STOP', 'STORM', 'T', 'TACTILE', 'TAPE', 'TASK', 'TAU', 'TAX', 'THETA', 'TIP', 'TO', 'TOP', 'TRADE', 'TRAIL', 'TRAITS', 'TrIP', 'TRIP', 'TUBE', 'UP', 'VAN', 'VIA', 'WAS', 'WASH', 'WAVE', 'WISH']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/krobasky/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /home/krobasky/prompt/repo/gpubs/src/../../v1/data/search_terms/filtered_terms.txt\n"
     ]
    }
   ],
   "source": [
    "# Create the filtered_terms.txt file\n",
    "final_terms = create_filtered_search_terms(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "589293a4-80db-4b77-b84b-ceb9142969a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338143"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check length of final terms\n",
    "len(final_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4af66106-c54f-411f-ac5f-707a39e7eb87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Directory: /home/krobasky/prompt/repo/gpubs/src/../../v1/data/raw/pubs/\n",
      "Number of abstracts to ensure have been downloaded: 3\n",
      "Refresh: False\n",
      "Total number of NCBI abstract XML files: 1166\n",
      "latest_files 3: ['pubmed23n1166.xml.gz', 'pubmed23n1165.xml.gz', 'pubmed23n1164.xml.gz']\n",
      "Predicted download size = 150MiB, Available space = 111GiB\n",
      "SKIP: /home/krobasky/prompt/repo/gpubs/src/../../v1/data/raw/pubs/pubmed23n1166.xml.gz exists.\n",
      "SKIP: /home/krobasky/prompt/repo/gpubs/src/../../v1/data/raw/pubs/pubmed23n1165.xml.gz exists.\n",
      "SKIP: /home/krobasky/prompt/repo/gpubs/src/../../v1/data/raw/pubs/pubmed23n1164.xml.gz exists.\n",
      "Total size of abstract files: 150MiB\n"
     ]
    }
   ],
   "source": [
    "# Fetch NCBI articl zips\n",
    "# - There are about 1100 files with about 15000 abstracts each.\n",
    "# - ~60GB is needed to get all files\n",
    "# - At about 2 min/file ... ~ 2 days to get 'em all\n",
    "m.num_abstract_xml_files=3 # set to -1 to get all files\n",
    "fetch_abstracts(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c808fdc6-67f2-4b35-957b-83b30eba667a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%bash\n",
    "# this would probably be faster, but harder to maintain\n",
    "#VERSION_ROOT=v1/data\n",
    "#VERBOSE=1\n",
    "#./gpubs/scripts/download_pubs.sh -n 5 -d ${VERSION_ROOT}/raw/pubs -v ${VERBOSE} 2> download.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ece86a0c-d677-44f0-aea2-77da07d6e131",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file /home/krobasky/prompt/repo/gpubs/src/../../v1/data/raw/pubs/pubmed23n1165.xml.gz\n",
      "Number of all articles:29996\n",
      "Number of all abstracts before pruning short articles = 25905\n",
      "Number after pruning short articles = 16511\n",
      "Number discarded for being too short: 9394\n",
      "Number of pruned articles:16511\n",
      "Wrote file:/home/krobasky/prompt/repo/gpubs/src/../../v1/data/csvpubs/pubmed23n1165.xml.gz.csv\n",
      "Converting file /home/krobasky/prompt/repo/gpubs/src/../../v1/data/raw/pubs/pubmed23n1166.xml.gz\n",
      "Number of all articles:10710\n",
      "Number of all abstracts before pruning short articles = 9250\n",
      "Number after pruning short articles = 5558\n",
      "Number discarded for being too short: 3692\n",
      "Number of pruned articles:5558\n",
      "Wrote file:/home/krobasky/prompt/repo/gpubs/src/../../v1/data/csvpubs/pubmed23n1166.xml.gz.csv\n",
      "Converting file /home/krobasky/prompt/repo/gpubs/src/../../v1/data/raw/pubs/pubmed23n1164.xml.gz\n",
      "Number of all articles:29986\n",
      "Number of all abstracts before pruning short articles = 26739\n",
      "Number after pruning short articles = 17326\n",
      "Number discarded for being too short: 9413\n",
      "Number of pruned articles:17326\n",
      "Wrote file:/home/krobasky/prompt/repo/gpubs/src/../../v1/data/csvpubs/pubmed23n1164.xml.gz.csv\n",
      "CPU times: user 34.9 s, sys: 1.23 s, total: 36.1 s\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create CSVs from XMLs\n",
    "# - This takes about 3 minutes to do 10 files; or about 5 hours to do them all\n",
    "# - Here we only need about a minute to do the 3 files we downloaded\n",
    "csv_list = create_pubcsv_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a606bbd2-189a-4750-94c2-34403b7a97ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating /home/krobasky/prompt/repo/gpubs/src/../../v1/data/csvpubs/genes/pubmed23n1165.xml.gz.csv\n",
      "Creating /home/krobasky/prompt/repo/gpubs/src/../../v1/data/csvpubs/genes/pubmed23n1164.xml.gz.csv\n",
      "Creating /home/krobasky/prompt/repo/gpubs/src/../../v1/data/csvpubs/genes/pubmed23n1166.xml.gz.csv\n",
      "CPU times: user 7.72 ms, sys: 69.7 ms, total: 77.4 ms\n",
      "Wall time: 5.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create new CSVs that include GENES column under data/csvpubs/genes\n",
    "# - Takes about 40s for 10 files, which is much slower than just running the awk script\n",
    "# - Here, it should only take a few seconds for the 3 files we downloaded\n",
    "# - With default settings, it filters out about 42% of the abstracts, most of which are 2022\n",
    "create_gene_files(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "be45dbe2-9511-402e-8844-2ef5545ee11f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%bash\n",
    "# This is SO much faster, but not as sustainable.\n",
    "#./gpubs/scripts/search.awk \\\n",
    "#  ./v4/data/search_terms/filtered_terms.txt \\\n",
    "#  ./v4/data/csvpubs/pubmed23n1166.xml.gz.csv \\\n",
    "#> ./v4/data/csvpubs/genes/pubmed23n1166.xml.gz.csv 2> ./v4/data/csvpubs/genes/pubmed23n1166.xml.gz.csv.err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b35aae1c-e781-4048-973a-cde6e12f2fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22932\n",
      "   17326 ../../v1/data/csvpubs/genes/pubmed23n1164.xml.gz.csv\n",
      "   16528 ../../v1/data/csvpubs/genes/pubmed23n1165.xml.gz.csv\n",
      "    5558 ../../v1/data/csvpubs/genes/pubmed23n1166.xml.gz.csv\n",
      "   39412 total\n"
     ]
    }
   ],
   "source": [
    "# Check your work\n",
    "# field 10 has the genes\n",
    "!awk -F'\\t' '$10 != \"\"{print $10}' ../../v1/data/csvpubs/genes/*.xml.gz.csv|wc -l\n",
    "! wc -l ../../v1/data/csvpubs/genes/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8b683327-41d5-463d-81a2-26e8a9c77967",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22932\n",
      "   17326 ../../v1/data/csvpubs/genes/pubmed23n1164.xml.gz.csv\n",
      "   16528 ../../v1/data/csvpubs/genes/pubmed23n1165.xml.gz.csv\n",
      "    5558 ../../v1/data/csvpubs/genes/pubmed23n1166.xml.gz.csv\n",
      "   39412 total\n",
      "maps\n",
      "SR\n",
      "MB\n",
      "toll\n",
      "ANOVA\n",
      "CT\n",
      "rim\n",
      "CT\n",
      "Dkk1\n",
      "MI\n",
      "AS,TNC\n",
      "DM\n",
      "GDF-15\n",
      "clock\n",
      "STAT3,T-bet,IL-17A,TSC1,TSC2,IL-17F,M1,LPS,IL-17,MTOR,TSC,DSS\n",
      "CD8,EGFR\n",
      "STING,cGAS\n",
      "APE1,GAD\n",
      "CD4,CCl\n",
      "Mb\n",
      "IV\n",
      "CI,HR\n",
      "tech\n",
      "MRS,SD\n",
      "CI\n",
      "AST\n",
      "RPE\n",
      "Cord,SCS,cord\n",
      "OT,ROM,grip\n",
      "AIS,DAO\n",
      "II\n",
      "CT\n",
      "TNT\n",
      "STR\n",
      "CT\n",
      "ASA\n",
      "DM,KSA,SD\n",
      "MIS\n",
      "AH,atopy\n",
      "CI\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Check your work\n",
    "# - If there are common words (like 'maps'), check in gene_info.gz if every occurrence is all-caps, and if so, add it to the custom_stop_words array in ReferenceData\n",
    "\n",
    "# field 10 has the genes\n",
    "awk -F'\\t' '$10 != \"\"{print $10}' ../../v1/data/csvpubs/genes/*.xml.gz.csv|wc -l\n",
    "wc -l ../../v1/data/csvpubs/genes/*.csv\n",
    "cat ../../v1/data/csvpubs/genes/pubmed23n1166.xml.gz.csv.err\n",
    "awk -F'\\t' '$10 != \"\" {print $10}' ../../v1/data/csvpubs/genes/pubmed23n1166.xml.gz.csv|head -120|tail -40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba32e2-3c68-454d-bcf2-df622db2a8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prompt]",
   "language": "python",
   "name": "conda-env-prompt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
